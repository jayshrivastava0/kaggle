{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"},{"sourceId":6478229,"sourceType":"datasetVersion","datasetId":3742543},{"sourceId":196651220,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":4148.03038,"end_time":"2024-09-27T08:52:29.611579","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-27T07:43:21.581199","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA","metadata":{"papermill":{"duration":0.012459,"end_time":"2024-09-27T07:43:24.384906","exception":false,"start_time":"2024-09-27T07:43:24.372447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import Basic Viz Libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"papermill":{"duration":2.824168,"end_time":"2024-09-27T07:43:27.224917","exception":false,"start_time":"2024-09-27T07:43:24.400749","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:17:26.066372Z","iopub.execute_input":"2024-09-30T21:17:26.066678Z","iopub.status.idle":"2024-09-30T21:17:29.101750Z","shell.execute_reply.started":"2024-09-30T21:17:26.066643Z","shell.execute_reply":"2024-09-30T21:17:29.100793Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e9/sample_submission.csv\n/kaggle/input/playground-series-s4e9/train.csv\n/kaggle/input/playground-series-s4e9/test.csv\n/kaggle/input/used-car-price-prediction-dataset/used_cars.csv\n/kaggle/input/top-5-blended-car-prices/__results__.html\n/kaggle/input/top-5-blended-car-prices/submission_8.csv\n/kaggle/input/top-5-blended-car-prices/submission_9.csv\n/kaggle/input/top-5-blended-car-prices/__notebook__.ipynb\n/kaggle/input/top-5-blended-car-prices/__output__.json\n/kaggle/input/top-5-blended-car-prices/custom.css\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.065074,"end_time":"2024-09-27T07:44:11.024622","exception":false,"start_time":"2024-09-27T07:44:10.959548","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -U autogluon","metadata":{"papermill":{"duration":224.900836,"end_time":"2024-09-27T07:47:55.991398","exception":false,"start_time":"2024-09-27T07:44:11.090562","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:19:42.241137Z","iopub.execute_input":"2024-09-30T21:19:42.241807Z","iopub.status.idle":"2024-09-30T21:23:27.852929Z","shell.execute_reply.started":"2024-09-30T21:19:42.241757Z","shell.execute_reply":"2024-09-30T21:23:27.851783Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Collecting autogluon\n  Downloading autogluon-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.core==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.features==1.1.1 (from autogluon)\n  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.tabular==1.1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\nCollecting autogluon.multimodal==1.1.1 (from autogluon)\n  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.timeseries==1.1.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\nCollecting scipy<1.13,>=1.5.4 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3)\nRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.2)\nRequirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.32.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.5)\nRequirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.100)\nCollecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting ray<2.11,>=2.10.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\nRequirement already satisfied: Pillow<11,>=10.0.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (10.3.0)\nCollecting torch<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\nCollecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\nCollecting jsonschema<4.22,>=4.18 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\nCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\nCollecting torchvision<0.19.0,>=0.16.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting scikit-image<0.21.0,>=0.19.1 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\nCollecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\nCollecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\nCollecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\nCollecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)\nCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nCollecting nltk<4.0.0,>=3.4.5 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\nRequirement already satisfied: tensorboard<3,>=2.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (2.16.2)\nCollecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\nCollecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (1.17.0)\nRequirement already satisfied: xgboost<2.1,>=1.6 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\nRequirement already satisfied: fastai<2.8,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.17)\nRequirement already satisfied: lightgbm<4.4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.2.0)\nRequirement already satisfied: catboost<1.3,>=1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.2.7)\nRequirement already satisfied: joblib<2,>=1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\nCollecting pytorch-lightning<2.4,>=2.2 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\nCollecting gluonts==0.15.1 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading gluonts-0.15.1-py3-none-any.whl.metadata (9.9 kB)\nCollecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)\nCollecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: orjson~=3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (3.10.4)\nCollecting optimum<1.19,>=1.17 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (70.0.0)\nRequirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.9.2)\nRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.2)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.6.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.25.0)\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (24.0)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.8,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.7.8)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (3.0.0)\nRequirement already satisfied: py4j in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.18.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (0.11.7)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.60.0)\nCollecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\nCollecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.5.15)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.4.6)\nCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.7.1)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\nCollecting coloredlogs (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.13.3)\nCollecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.16.2)\nCollecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.15.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\nRequirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.9.5)\nCollecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.5.6)\nRequirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.14)\nRequirement already satisfied: opencensus in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.11.4)\nRequirement already satisfied: prometheus-client>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.20.0)\nRequirement already satisfied: smart-open in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (20.21.0)\nRequirement already satisfied: grpcio>=1.42.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.62.2)\nRequirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.6.2.2)\nRequirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (16.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.8.30)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.34.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.5.22)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.6.0)\nRequirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.5)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting tokenizers<0.19,>=0.14 (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.43.0)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.23.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\nRequirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.8)\nRequirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.11.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.11.1)\nRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.20.0)\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (8.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.18.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.16.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.63.1)\nRequirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.30.0)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.4)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.19.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.5)\nCollecting filelock (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting packaging>=20.0 (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting pytz>=2020.1 (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\nINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\nINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.9)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.6.0)\nDownloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\nDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.3.3-py3-none-any.whl (808 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nptyping-2.4.1-py3-none-any.whl (36 kB)\nDownloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading optimum-1.18.1-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\nDownloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\nDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\nDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\nDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\nBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=ed2ef1018258f8ddc9b26efad62c22241df9929d1fea727e8a275377502356f3\n  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=614c23ce122bc4617480acbd3b735641fc942fba224fc24198e01c7bafd29485\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=c373de9449424697856eb4c956573188e71c3f60811fd31da5ddb3caac8a6647\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\nInstalling collected packages: nvidia-ml-py3, antlr4-python3-runtime, triton, scipy, ordered-set, openxlab, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, nltk, humanfriendly, window-ops, scikit-learn, pytesseract, nvidia-cusparse-cu12, nvidia-cudnn-cu12, model-index, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, scikit-image, opendatalab, onnxruntime, nvidia-cusolver-cu12, jsonschema, gluonts, gdown, aiohttp-cors, transformers, torch, statsforecast, ray, openmim, nlpaug, mlforecast, torchvision, torchmetrics, pytorch-metric-learning, evaluate, autogluon.common, accelerate, timm, pytorch-lightning, optimum, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: pytesseract\n    Found existing installation: pytesseract 0.3.13\n    Uninstalling pytesseract-0.3.13:\n      Successfully uninstalled pytesseract-0.3.13\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.35.16\n    Uninstalling botocore-1.35.16:\n      Successfully uninstalled botocore-1.35.16\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: scikit-image\n    Found existing installation: scikit-image 0.23.2\n    Uninstalling scikit-image-0.23.2:\n      Successfully uninstalled scikit-image-0.23.2\n  Attempting uninstall: jsonschema\n    Found existing installation: jsonschema 4.22.0\n    Uninstalling jsonschema-4.22.0:\n      Successfully uninstalled jsonschema-4.22.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n  Attempting uninstall: ray\n    Found existing installation: ray 2.24.0\n    Uninstalling ray-2.24.0:\n      Successfully uninstalled ray-2.24.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.4.2\n    Uninstalling torchmetrics-1.4.2:\n      Successfully uninstalled torchmetrics-1.4.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.4.0\n    Uninstalling pytorch-lightning-2.4.0:\n      Successfully uninstalled pytorch-lightning-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\naiobotocore 2.15.0 requires botocore<1.35.17,>=1.35.16, but you have botocore 1.29.165 which is incompatible.\nalbumentations 1.4.15 requires scikit-image>=0.21.0, but you have scikit-image 0.20.0 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 botocore-1.29.165 coloredlogs-15.0.1 evaluate-0.4.3 gdown-5.2.0 gluonts-0.15.1 humanfriendly-10.0 jsonschema-4.21.1 lightning-2.3.3 mlforecast-0.10.0 model-index-0.1.11 nlpaug-1.1.11 nltk-3.9.1 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnxruntime-1.19.2 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 pytesseract-0.3.10 pytorch-lightning-2.3.3 pytorch-metric-learning-2.3.0 ray-2.10.0 scikit-image-0.20.0 scikit-learn-1.4.0 scipy-1.12.0 seqeval-1.2.2 statsforecast-1.4.0 timm-0.9.16 tokenizers-0.15.2 torch-2.3.1 torchmetrics-1.2.1 torchvision-0.18.1 transformers-4.39.3 triton-2.3.1 utilsforecast-0.0.10 window-ops-0.0.15\n","output_type":"stream"}]},{"cell_type":"code","source":"from autogluon.tabular import TabularPredictor\nimport re\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv')\nOriginal = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset/used_cars.csv')\nOriginal[['milage', 'price']] = Original[['milage', 'price']].map(\n    lambda x: int(''.join(re.findall(r'\\d+', x))))\n\nimport lightgbm as lgb\nfrom lightgbm import log_evaluation, early_stopping\nfrom catboost import CatBoostRegressor, Pool\n\nfrom xgboost import XGBRegressor\n\n\nimport random\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nfrom autogluon.tabular import TabularPredictor","metadata":{"papermill":{"duration":6.055247,"end_time":"2024-09-27T07:48:02.214768","exception":false,"start_time":"2024-09-27T07:47:56.159521","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:25:16.109922Z","iopub.execute_input":"2024-09-30T21:25:16.110315Z","iopub.status.idle":"2024-09-30T21:25:16.964862Z","shell.execute_reply.started":"2024-09-30T21:25:16.110276Z","shell.execute_reply":"2024-09-30T21:25:16.963864Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv')\nOriginal = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset/used_cars.csv')\nOriginal[['milage', 'price']] = Original[['milage', 'price']].map(\n    lambda x: int(''.join(re.findall(r'\\d+', x))))","metadata":{"papermill":{"duration":1.088702,"end_time":"2024-09-27T07:48:03.462945","exception":false,"start_time":"2024-09-27T07:48:02.374243","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:25:17.862469Z","iopub.execute_input":"2024-09-30T21:25:17.862857Z","iopub.status.idle":"2024-09-30T21:25:18.689082Z","shell.execute_reply.started":"2024-09-30T21:25:17.862818Z","shell.execute_reply":"2024-09-30T21:25:18.688294Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=['id'], inplace=True)\ntest.drop(columns=['id'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:18.690565Z","iopub.execute_input":"2024-09-30T21:25:18.690857Z","iopub.status.idle":"2024-09-30T21:25:18.727628Z","shell.execute_reply.started":"2024-09-30T21:25:18.690825Z","shell.execute_reply":"2024-09-30T21:25:18.726876Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, Original], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:18.836865Z","iopub.execute_input":"2024-09-30T21:25:18.837187Z","iopub.status.idle":"2024-09-30T21:25:18.855107Z","shell.execute_reply.started":"2024-09-30T21:25:18.837152Z","shell.execute_reply":"2024-09-30T21:25:18.854304Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def extract_age_features(df):\n    current_year = 2024\n\n    df['Vehicle_Age'] = current_year - df['model_year']\n    \n    df['Mileage_per_Year'] = df['milage'] / df['Vehicle_Age']\n    df['milage_with_age'] =  df.groupby('Vehicle_Age')['milage'].transform('mean')\n    \n    df['Mileage_per_Year_with_age'] =  df.groupby('Vehicle_Age')['Mileage_per_Year'].transform('mean')\n    \n    return df\n\n\ndef extract_other_features(df):\n    \n    luxury_brands =  ['Mercedes-Benz', 'BMW', 'Audi', 'Porsche', 'Land', \n                    'Lexus', 'Jaguar', 'Bentley', 'Maserati', 'Lamborghini', \n                    'Rolls-Royce', 'Ferrari', 'McLaren', 'Aston', 'Maybach']\n    df['Is_Luxury_Brand'] = df['brand'].apply(lambda x: 1 if x in luxury_brands else 0)\n    \n\n    return df","metadata":{"papermill":{"duration":0.448324,"end_time":"2024-09-27T07:48:04.071783","exception":false,"start_time":"2024-09-27T07:48:03.623459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:25:19.359634Z","iopub.execute_input":"2024-09-30T21:25:19.359975Z","iopub.status.idle":"2024-09-30T21:25:19.367436Z","shell.execute_reply.started":"2024-09-30T21:25:19.359940Z","shell.execute_reply":"2024-09-30T21:25:19.366448Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train = extract_age_features(train)\ntest = extract_age_features(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:19.983849Z","iopub.execute_input":"2024-09-30T21:25:19.984581Z","iopub.status.idle":"2024-09-30T21:25:20.010572Z","shell.execute_reply.started":"2024-09-30T21:25:19.984541Z","shell.execute_reply":"2024-09-30T21:25:20.009829Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train = extract_other_features(train)\ntest = extract_other_features(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:20.358587Z","iopub.execute_input":"2024-09-30T21:25:20.358907Z","iopub.status.idle":"2024-09-30T21:25:20.602071Z","shell.execute_reply.started":"2024-09-30T21:25:20.358874Z","shell.execute_reply":"2024-09-30T21:25:20.601374Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def update(df):\n    \n    t = 100\n    \n    cat_c = ['brand','model','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title',\n             \n            ]\n    re_ = ['model','engine','transmission','ext_col','int_col']\n    \n    for col in re_:\n        df.loc[df[col].value_counts(dropna=False)[df[col]].values < t, col] = \"noise\"\n        \n    for col in cat_c:\n        df[col] = df[col].fillna('missing')\n        df[col] = df[col].astype('category')\n        \n    return df","metadata":{"papermill":{"duration":1.14199,"end_time":"2024-09-27T07:48:05.370594","exception":false,"start_time":"2024-09-27T07:48:04.228604","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:25:21.134576Z","iopub.execute_input":"2024-09-30T21:25:21.134904Z","iopub.status.idle":"2024-09-30T21:25:21.141432Z","shell.execute_reply.started":"2024-09-30T21:25:21.134872Z","shell.execute_reply":"2024-09-30T21:25:21.140480Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"train  = update(train)\ntest   = update(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:21.532054Z","iopub.execute_input":"2024-09-30T21:25:21.532468Z","iopub.status.idle":"2024-09-30T21:25:22.500073Z","shell.execute_reply.started":"2024-09-30T21:25:21.532431Z","shell.execute_reply":"2024-09-30T21:25:22.499299Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"X = train.drop('price', axis=1)\ny = train['price']","metadata":{"execution":{"iopub.status.busy":"2024-09-30T21:25:22.501504Z","iopub.execute_input":"2024-09-30T21:25:22.501810Z","iopub.status.idle":"2024-09-30T21:25:22.511995Z","shell.execute_reply.started":"2024-09-30T21:25:22.501776Z","shell.execute_reply":"2024-09-30T21:25:22.511222Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n\n\ncallbacks = [log_evaluation(period=300), early_stopping(stopping_rounds=200)]\n\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(f\"cat_cols--------{cat_cols}\")\n\n\ndef get_MAE_oof(df, target, lgb_params, cat_params=None, model_type='LGBM'):\n\n    \n    oof_predictions = np.zeros(len(df))\n    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n    models = []\n    rmse_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n        print(f\"Training fold {fold + 1}/{5} with {model_type}\")\n\n        X_train, X_val = df.iloc[train_idx], df.iloc[val_idx]\n        y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n\n        if model_type == 'LGBM':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n            \n            model = lgb.train(\n                lgb_params,\n                train_data,\n                valid_sets=[train_data, val_data],\n                valid_names=['train', 'valid'],\n                callbacks=callbacks    \n            )\n        \n        elif model_type == 'CAT':\n            train_data = Pool(data=X_train, label=y_train , cat_features=cat_cols)\n            val_data = Pool(data=X_val, label=y_val , cat_features=cat_cols )\n            \n            model = CatBoostRegressor(**cat_params)\n            model.fit(train_data, eval_set=val_data, verbose=150, early_stopping_rounds=200)\n        \n        models.append(model)\n        \n        if model_type == 'LGBM':\n            pred = model.predict(X_val, num_iteration=model.best_iteration)\n        elif model_type == 'CAT':\n            pred = model.predict(X_val)\n        \n        rmse = np.sqrt(mean_squared_error(y_val, pred))\n        rmse_scores.append(rmse)\n\n        print(f'{model_type} Fold RMSE: {rmse}')\n        \n        oof_predictions[val_idx] = pred\n        \n    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n    return oof_predictions, models\n\n\n\n\nlgb_params = {\n    'objective': 'MAE',\n    \"learning_rate\": 0.012108664290953471,\n    \"max_depth\": 9,\n    \"reg_alpha\": 8.78291059572591,\n    \"reg_lambda\": 2.091929010378848,\n    \"num_leaves\": 61,\n    \"subsample\": 0.3550284549842191,\n    \"colsample_bytree\": 0.3403791928659016,\n    \"min_child_samples\": 41,\n    \"min_split_gain\": 0.017167400925454185,\n    \"n_estimators\": 1000,\n    \"random_state\": 42,\n}\n\noof_predictions_lgbm, models_lgbm = get_MAE_oof(X, y, lgb_params, model_type='LGBM')\nX['LGBM_MAE'] = oof_predictions_lgbm\n\n\nLGBM_preds = np.zeros(len(test))\nfor model in models_lgbm:\n    LGBM_preds += model.predict(test) / len(models_lgbm)\ntest['LGBM_MAE'] = LGBM_preds\n\n\n\nlgb_params = {\n    'objective': 'MSE',\n    \"learning_rate\": 0.012108664290953471,\n    \"max_depth\": 9,\n    \"reg_alpha\": 8.78291059572591,\n    \"reg_lambda\": 2.091929010378848,\n    \"num_leaves\": 61,\n    \"subsample\": 0.3550284549842191,\n    \"colsample_bytree\": 0.3403791928659016,\n    \"min_child_samples\": 41,\n    \"min_split_gain\": 0.017167400925454185,\n    \"n_estimators\": 1000,\n    \"random_state\": 42,\n}\n\noof_predictions_lgbm, models_lgbm = get_MAE_oof(X, y, lgb_params, model_type='LGBM')\n\nX['LGBM_MSE_diff'] = oof_predictions_lgbm - X['LGBM_MAE']\n\n\nLGBM_preds = np.zeros(len(test))\nfor model in models_lgbm:\n    LGBM_preds += model.predict(test) / len(models_lgbm)\ntest['LGBM_MSE_diff'] = LGBM_preds - test['LGBM_MAE']\n\ntest.head()","metadata":{"papermill":{"duration":226.470774,"end_time":"2024-09-27T07:51:51.998599","exception":false,"start_time":"2024-09-27T07:48:05.527825","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T21:25:25.511766Z","iopub.execute_input":"2024-09-30T21:25:25.512174Z","iopub.status.idle":"2024-09-30T21:27:46.564946Z","shell.execute_reply.started":"2024-09-30T21:25:25.512134Z","shell.execute_reply":"2024-09-30T21:27:46.564096Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"cat_cols--------['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\nTraining fold 1/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1724\n[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 16\n[LightGBM] [Info] Start training from score 30825.000000\nTraining until validation scores don't improve for 200 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttrain's l1: 17202.4\tvalid's l1: 17089.9\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\ttrain's l1: 16887.7\tvalid's l1: 16894.5\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[900]\ttrain's l1: 16751.7\tvalid's l1: 16846\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nDid not meet early stopping. Best iteration is:\n[1000]\ttrain's l1: 16718.1\tvalid's l1: 16838.1\nLGBM Fold RMSE: 67902.6617213504\nTraining fold 2/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015131 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1724\n[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 16\n[LightGBM] [Info] Start training from score 30900.000000\nTraining until validation scores don't improve for 200 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttrain's l1: 16991.7\tvalid's l1: 17919.4\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\ttrain's l1: 16668.9\tvalid's l1: 17734.1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[900]\ttrain's l1: 16539.3\tvalid's l1: 17691.2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nDid not meet early stopping. Best iteration is:\n[1000]\ttrain's l1: 16505.3\tvalid's l1: 17682.4\nLGBM Fold RMSE: 85066.6757742869\nTraining fold 3/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014298 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1723\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n[LightGBM] [Info] Start training from score 30798.000000\nTraining until validation scores don't improve for 200 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttrain's l1: 17025.8\tvalid's l1: 17829.6\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\ttrain's l1: 16717.4\tvalid's l1: 17590.2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[900]\ttrain's l1: 16584.3\tvalid's l1: 17526\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nDid not meet early stopping. Best iteration is:\n[1000]\ttrain's l1: 16553.2\tvalid's l1: 17514.7\nLGBM Fold RMSE: 76723.68724819807\nTraining fold 4/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1724\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n[LightGBM] [Info] Start training from score 30900.000000\nTraining until validation scores don't improve for 200 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttrain's l1: 17267.2\tvalid's l1: 16828\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\ttrain's l1: 16943.3\tvalid's l1: 16625.5\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[900]\ttrain's l1: 16815\tvalid's l1: 16581.3\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nDid not meet early stopping. Best iteration is:\n[1000]\ttrain's l1: 16783.7\tvalid's l1: 16574.1\nLGBM Fold RMSE: 66975.04285666779\nTraining fold 5/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1723\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n[LightGBM] [Info] Start training from score 30798.000000\nTraining until validation scores don't improve for 200 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttrain's l1: 17214.5\tvalid's l1: 17050.2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\ttrain's l1: 16895.9\tvalid's l1: 16843.2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[900]\ttrain's l1: 16767.7\tvalid's l1: 16807.2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nDid not meet early stopping. Best iteration is:\n[1000]\ttrain's l1: 16733.2\tvalid's l1: 16798.7\nLGBM Fold RMSE: 70141.5343330631\nMean RMSE: 73361.92038671326\nTraining fold 1/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005163 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1979\n[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 17\n[LightGBM] [Info] Start training from score 43958.172307\nTraining until validation scores don't improve for 200 rounds\n[300]\ttrain's l2: 4.94181e+09\tvalid's l2: 4.50228e+09\nEarly stopping, best iteration is:\n[255]\ttrain's l2: 5.00647e+09\tvalid's l2: 4.49833e+09\nLGBM Fold RMSE: 67069.61116203602\nTraining fold 2/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005168 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1979\n[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 17\n[LightGBM] [Info] Start training from score 43756.952828\nTraining until validation scores don't improve for 200 rounds\n[300]\ttrain's l2: 4.372e+09\tvalid's l2: 7.05865e+09\nEarly stopping, best iteration is:\n[266]\ttrain's l2: 4.41731e+09\tvalid's l2: 7.05547e+09\nLGBM Fold RMSE: 83996.85768678438\nTraining fold 3/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009573 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1978\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n[LightGBM] [Info] Start training from score 43759.840003\nTraining until validation scores don't improve for 200 rounds\n[300]\ttrain's l2: 4.67243e+09\tvalid's l2: 5.6958e+09\nEarly stopping, best iteration is:\n[352]\ttrain's l2: 4.61912e+09\tvalid's l2: 5.69458e+09\nLGBM Fold RMSE: 75462.41593830808\nTraining fold 4/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005323 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1979\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n[LightGBM] [Info] Start training from score 44054.894439\nTraining until validation scores don't improve for 200 rounds\n[300]\ttrain's l2: 4.97933e+09\tvalid's l2: 4.37521e+09\nEarly stopping, best iteration is:\n[231]\ttrain's l2: 5.08055e+09\tvalid's l2: 4.36991e+09\nLGBM Fold RMSE: 66105.26260557375\nTraining fold 5/5 with LGBM\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005032 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1978\n[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n[LightGBM] [Info] Start training from score 43930.511322\nTraining until validation scores don't improve for 200 rounds\n[300]\ttrain's l2: 4.88822e+09\tvalid's l2: 4.77485e+09\nEarly stopping, best iteration is:\n[270]\ttrain's l2: 4.93022e+09\tvalid's l2: 4.77084e+09\nLGBM Fold RMSE: 69071.29544939227\nMean RMSE: 72341.0885684189\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"  brand                 model  model_year  milage fuel_type  \\\n0  Land                 noise        2015   98000  Gasoline   \n1  Land     Rover Defender SE        2020    9142    Hybrid   \n2  Ford    Expedition Limited        2022   28121  Gasoline   \n3  Audi                 noise        2016   61258  Gasoline   \n4  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n\n                                              engine        transmission  \\\n0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n3                                     2.0 Liter TFSI           Automatic   \n4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n\n  ext_col int_col       accident clean_title  Vehicle_Age  Mileage_per_Year  \\\n0   White   Beige  None reported         Yes            9      10888.888889   \n1  Silver   Black  None reported         Yes            4       2285.500000   \n2   White   Ebony  None reported     missing            2      14060.500000   \n3   noise   Black  None reported     missing            8       7657.250000   \n4    Gray   Black  None reported         Yes            6       9833.333333   \n\n   milage_with_age  Mileage_per_Year_with_age  Is_Luxury_Brand      LGBM_MAE  \\\n0     81078.503981                9008.722665                1  16550.121430   \n1     34258.886442                8564.721611                1  57325.684034   \n2     17877.043403                8938.521702                0  50667.051811   \n3     75999.679762                9499.959970                1  23411.453878   \n4     52105.532436                8684.255406                1  26507.056173   \n\n   LGBM_MSE_diff  \n0    2958.792366  \n1   15896.655181  \n2    5382.961318  \n3    4704.911733  \n4    4972.793843  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>brand</th>\n      <th>model</th>\n      <th>model_year</th>\n      <th>milage</th>\n      <th>fuel_type</th>\n      <th>engine</th>\n      <th>transmission</th>\n      <th>ext_col</th>\n      <th>int_col</th>\n      <th>accident</th>\n      <th>clean_title</th>\n      <th>Vehicle_Age</th>\n      <th>Mileage_per_Year</th>\n      <th>milage_with_age</th>\n      <th>Mileage_per_Year_with_age</th>\n      <th>Is_Luxury_Brand</th>\n      <th>LGBM_MAE</th>\n      <th>LGBM_MSE_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Land</td>\n      <td>noise</td>\n      <td>2015</td>\n      <td>98000</td>\n      <td>Gasoline</td>\n      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>6-Speed A/T</td>\n      <td>White</td>\n      <td>Beige</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>9</td>\n      <td>10888.888889</td>\n      <td>81078.503981</td>\n      <td>9008.722665</td>\n      <td>1</td>\n      <td>16550.121430</td>\n      <td>2958.792366</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Land</td>\n      <td>Rover Defender SE</td>\n      <td>2020</td>\n      <td>9142</td>\n      <td>Hybrid</td>\n      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...</td>\n      <td>8-Speed A/T</td>\n      <td>Silver</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>4</td>\n      <td>2285.500000</td>\n      <td>34258.886442</td>\n      <td>8564.721611</td>\n      <td>1</td>\n      <td>57325.684034</td>\n      <td>15896.655181</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ford</td>\n      <td>Expedition Limited</td>\n      <td>2022</td>\n      <td>28121</td>\n      <td>Gasoline</td>\n      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n      <td>10-Speed Automatic</td>\n      <td>White</td>\n      <td>Ebony</td>\n      <td>None reported</td>\n      <td>missing</td>\n      <td>2</td>\n      <td>14060.500000</td>\n      <td>17877.043403</td>\n      <td>8938.521702</td>\n      <td>0</td>\n      <td>50667.051811</td>\n      <td>5382.961318</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Audi</td>\n      <td>noise</td>\n      <td>2016</td>\n      <td>61258</td>\n      <td>Gasoline</td>\n      <td>2.0 Liter TFSI</td>\n      <td>Automatic</td>\n      <td>noise</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>missing</td>\n      <td>8</td>\n      <td>7657.250000</td>\n      <td>75999.679762</td>\n      <td>9499.959970</td>\n      <td>1</td>\n      <td>23411.453878</td>\n      <td>4704.911733</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Audi</td>\n      <td>A6 2.0T Premium Plus</td>\n      <td>2018</td>\n      <td>59000</td>\n      <td>Gasoline</td>\n      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>A/T</td>\n      <td>Gray</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>6</td>\n      <td>9833.333333</td>\n      <td>52105.532436</td>\n      <td>8684.255406</td>\n      <td>1</td>\n      <td>26507.056173</td>\n      <td>4972.793843</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X['price'] = y\n\n# Fit the predictor\npredictor = TabularPredictor(label='price',\n                             eval_metric='rmse',\n                             problem_type='regression').fit(\n    X,\n    presets='best_quality',\n    time_limit=3600*0.5,\n    verbosity=2,\n    num_gpus=0,\n    included_model_types=['GBM', 'CAT'],  # Boosting models\n)","metadata":{"papermill":{"duration":3603.857821,"end_time":"2024-09-27T08:51:56.05499","exception":false,"start_time":"2024-09-27T07:51:52.197169","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-30T22:32:01.318312Z","iopub.execute_input":"2024-09-30T22:32:01.318711Z","iopub.status.idle":"2024-09-30T23:01:52.617233Z","shell.execute_reply.started":"2024-09-30T22:32:01.318675Z","shell.execute_reply":"2024-09-30T23:01:52.616315Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240930_223201\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.14\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       28.92 GB / 31.36 GB (92.2%)\nDisk Space Avail:   19.13 GB / 19.52 GB (98.0%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 450s of the 1800.0s of remaining time (25%).\n\t\tContext path: \"AutogluonModels/ag-20240930_223201/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                   model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0        CatBoost_BAG_L1  -69316.270994 -72935.164508  root_mean_squared_error        0.750637       0.632163  208.161564                 0.750637                0.632163         208.161564            1       True          3\n1    WeightedEnsemble_L3  -69331.721325 -72877.385246  root_mean_squared_error        5.056496       7.554647  411.201153                 0.004203                0.003173           0.247263            3       True         11\n2    WeightedEnsemble_L2  -69344.843151 -72882.498926  root_mean_squared_error        3.105970       2.236468  228.089487                 0.002632                0.003306           0.210628            2       True          6\n3        LightGBM_BAG_L2  -69400.565258 -73202.190084  root_mean_squared_error        4.432089       5.646631  306.792118                 0.310856                0.903133          21.350489            2       True          8\n4        CatBoost_BAG_L2  -69407.930140 -72938.574427  root_mean_squared_error        4.341776       5.245376  367.305228                 0.220543                0.501878          81.863599            2       True          9\n5      LightGBMXT_BAG_L2  -69453.933484 -73018.145172  root_mean_squared_error        4.417957       6.124732  305.874486                 0.296724                1.381235          20.432858            2       True          7\n6        LightGBM_BAG_L1  -69464.349705 -73271.318924  root_mean_squared_error        0.308535       0.763037   17.685488                 0.308535                0.763037          17.685488            1       True          2\n7   LightGBMLarge_BAG_L2  -69484.585657 -73650.256273  root_mean_squared_error        4.520895       6.146463  307.739802                 0.399662                1.402965          22.298173            2       True         10\n8      LightGBMXT_BAG_L1  -69496.898059 -72973.936066  root_mean_squared_error        2.352702       1.600999   19.717295                 2.352702                1.600999          19.717295            1       True          1\n9   LightGBMLarge_BAG_L1  -69590.168011 -73817.744622  root_mean_squared_error        0.604617       1.348737   21.597161                 0.604617                1.348737          21.597161            1       True          4\n10  CatBoost_r177_BAG_L1  -70702.994819 -74405.545698  root_mean_squared_error        0.104742       0.398561   18.280121                 0.104742                0.398561          18.280121            1       True          5\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t471s\t = DyStack   runtime |\t1329s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 1329s\nAutoGluon will save models to \"AutogluonModels/ag-20240930_223201\"\nTrain Data Rows:    192542\nTrain Data Columns: 18\nLabel Column:       price\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29336.19 MB\n\tTrain Data (Original)  Memory Usage: 15.33 MB (0.1% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('category', []) : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n\t\t('float', [])    : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n\t\t('int', [])      : 4 | ['model_year', 'milage', 'Vehicle_Age', 'Is_Luxury_Brand']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n\t\t('float', [])     : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n\t\t('int', [])       : 3 | ['model_year', 'milage', 'Vehicle_Age']\n\t\t('int', ['bool']) : 2 | ['clean_title', 'Is_Luxury_Brand']\n\t0.5s = Fit runtime\n\t18 features in original data used to generate 18 features in processed data.\n\tTrain Data (Processed) Memory Usage: 13.96 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.53s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nIncluded models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\nFitting 36 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 885.62s of the 1328.75s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.31%)\n\t-72622.7574\t = Validation score   (-root_mean_squared_error)\n\t22.55s\t = Training   runtime\n\t1.5s\t = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 859.78s of the 1302.91s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.31%)\n\t-72782.8553\t = Validation score   (-root_mean_squared_error)\n\t22.04s\t = Training   runtime\n\t0.97s\t = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 834.42s of the 1277.55s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.33%)\n\t-72541.9622\t = Validation score   (-root_mean_squared_error)\n\t245.26s\t = Training   runtime\n\t0.59s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 585.52s of the 1028.65s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.35%)\n\t-73261.6685\t = Validation score   (-root_mean_squared_error)\n\t23.53s\t = Training   runtime\n\t1.45s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 558.71s of the 1001.84s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.33%)\n\t-72553.7983\t = Validation score   (-root_mean_squared_error)\n\t158.47s\t = Training   runtime\n\t0.44s\t = Validation runtime\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 397.11s of the 840.25s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.33%)\n\t-72752.2837\t = Validation score   (-root_mean_squared_error)\n\t48.66s\t = Training   runtime\n\t7.0s\t = Validation runtime\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 344.47s of the 787.6s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.39%)\n\t-72578.0772\t = Validation score   (-root_mean_squared_error)\n\t111.82s\t = Training   runtime\n\t0.79s\t = Validation runtime\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 229.53s of the 672.66s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.31%)\n\t-72606.7911\t = Validation score   (-root_mean_squared_error)\n\t58.9s\t = Training   runtime\n\t10.2s\t = Validation runtime\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 164.35s of the 607.49s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.31%)\n\t-72546.1387\t = Validation score   (-root_mean_squared_error)\n\t135.0s\t = Training   runtime\n\t0.57s\t = Validation runtime\nFitting model: CatBoost_r13_BAG_L1 ... Training model for up to 26.16s of the 469.29s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.41%)\n\t-77062.5469\t = Validation score   (-root_mean_squared_error)\n\t23.47s\t = Training   runtime\n\t0.34s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 442.51s of remaining time.\n\tEnsemble Weights: {'CatBoost_r9_BAG_L1': 0.263, 'CatBoost_BAG_L1': 0.211, 'LightGBM_r96_BAG_L1': 0.211, 'LightGBM_r131_BAG_L1': 0.158, 'CatBoost_r137_BAG_L1': 0.158}\n\t-72486.1152\t = Validation score   (-root_mean_squared_error)\n\t0.31s\t = Training   runtime\n\t0.0s\t = Validation runtime\nIncluded models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\nFitting 36 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 442.16s of the 442.12s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.47%)\n\t-72590.0473\t = Validation score   (-root_mean_squared_error)\n\t25.26s\t = Training   runtime\n\t1.11s\t = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 413.52s of the 413.47s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.48%)\n\t-72881.7074\t = Validation score   (-root_mean_squared_error)\n\t25.68s\t = Training   runtime\n\t0.93s\t = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 384.56s of the 384.51s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.50%)\n\t-72559.9984\t = Validation score   (-root_mean_squared_error)\n\t269.24s\t = Training   runtime\n\t0.52s\t = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 112.04s of the 111.99s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.54%)\n\t-73444.4935\t = Validation score   (-root_mean_squared_error)\n\t30.38s\t = Training   runtime\n\t1.39s\t = Validation runtime\nFitting model: CatBoost_r177_BAG_L2 ... Training model for up to 78.35s of the 78.3s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.50%)\n\t-72594.2381\t = Validation score   (-root_mean_squared_error)\n\t65.56s\t = Training   runtime\n\t0.38s\t = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 9.13s of remaining time.\n\tEnsemble Weights: {'CatBoost_r9_BAG_L1': 0.25, 'LightGBMXT_BAG_L2': 0.208, 'CatBoost_BAG_L1': 0.167, 'LightGBM_r131_BAG_L1': 0.125, 'LightGBM_r96_BAG_L1': 0.125, 'CatBoost_r137_BAG_L1': 0.125}\n\t-72480.0142\t = Validation score   (-root_mean_squared_error)\n\t0.32s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 1320.55s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 963.8 rows/s (24068 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240930_223201\")\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = predictor.predict(test)\npd.DataFrame(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T23:03:11.276352Z","iopub.execute_input":"2024-09-30T23:03:11.277446Z","iopub.status.idle":"2024-09-30T23:03:57.482967Z","shell.execute_reply.started":"2024-09-30T23:03:11.277399Z","shell.execute_reply":"2024-09-30T23:03:57.481935Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"               price\n0       19230.089844\n1       73318.523438\n2       59289.453125\n3       28202.111328\n4       30820.166016\n...              ...\n125685  27417.781250\n125686  54537.902344\n125687  20460.451172\n125688  15931.299805\n125689  42073.390625\n\n[125690 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19230.089844</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>73318.523438</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59289.453125</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28202.111328</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30820.166016</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125685</th>\n      <td>27417.781250</td>\n    </tr>\n    <tr>\n      <th>125686</th>\n      <td>54537.902344</td>\n    </tr>\n    <tr>\n      <th>125687</th>\n      <td>20460.451172</td>\n    </tr>\n    <tr>\n      <th>125688</th>\n      <td>15931.299805</td>\n    </tr>\n    <tr>\n      <th>125689</th>\n      <td>42073.390625</td>\n    </tr>\n  </tbody>\n</table>\n<p>125690 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub_blend = pd.read_csv('/kaggle/input/top-5-blended-car-prices/submission_9.csv')\nsample_sub = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T23:04:29.077032Z","iopub.execute_input":"2024-09-30T23:04:29.077803Z","iopub.status.idle":"2024-09-30T23:04:29.157030Z","shell.execute_reply.started":"2024-09-30T23:04:29.077761Z","shell.execute_reply":"2024-09-30T23:04:29.156288Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"sample_sub['price'] =  y_pred * 1 + sub_blend['price'] * 0.0\nsample_sub.to_csv(\"submission.csv\", index=False)\nsample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T23:05:27.688369Z","iopub.execute_input":"2024-09-30T23:05:27.688799Z","iopub.status.idle":"2024-09-30T23:05:28.106509Z","shell.execute_reply.started":"2024-09-30T23:05:27.688756Z","shell.execute_reply":"2024-09-30T23:05:28.104787Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  \u001b[43my_pred\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m sub_blend[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      2\u001b[0m sample_sub\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m sample_sub\u001b[38;5;241m.\u001b[39mhead()\n","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"],"ename":"NameError","evalue":"name 'y_pred' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}