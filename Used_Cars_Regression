{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T03:58:12.295106Z","iopub.execute_input":"2024-09-15T03:58:12.296166Z","iopub.status.idle":"2024-09-15T03:58:12.733700Z","shell.execute_reply.started":"2024-09-15T03:58:12.296106Z","shell.execute_reply":"2024-09-15T03:58:12.732167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport shap\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nimport lightgbm as lgb\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:12.736001Z","iopub.execute_input":"2024-09-15T03:58:12.736730Z","iopub.status.idle":"2024-09-15T03:58:22.639045Z","shell.execute_reply.started":"2024-09-15T03:58:12.736669Z","shell.execute_reply":"2024-09-15T03:58:22.637637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile data_preprocessing.py\n\nimport os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nimport numpy as np\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\nclass DataPreprocessor:\n    def __init__(self, train_file, test_file=None, rare_threshold=40, additional_categorical=None):\n        \"\"\"\n        Initialize the DataPreprocessor.\n\n        Parameters:\n        - train_file (str): Path to the training CSV file.\n        - test_file (str, optional): Path to the testing CSV file.\n        - rare_threshold (int): Threshold below which categories are considered rare.\n        - additional_categorical (list, optional): List of additional categorical column names.\n        \"\"\"\n        self.train_file = train_file\n        self.test_file = test_file\n        self.train = None\n        self.test = None\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n        self.rare_threshold = rare_threshold\n        self.CAT_SIZE = []\n        self.CAT_EMB = []\n        self.RARE = []\n        self.CATS = []  # List of categorical columns to encode\n        self.NUMS = []  # List of numerical columns to scale\n        self.additional_categorical = additional_categorical if additional_categorical else []\n\n    def load_data(self):\n        self.train = pl.read_csv(self.train_file)\n        if self.test_file:\n            self.test = pl.read_csv(self.test_file)\n            print(\"Training and Testing data loaded successfully.\")\n        else:\n            print(\"Training data loaded successfully.\")\n    \n    def replace_null_values(self):\n        # Replace specific null representations with \"Unknown\"\n        for column in [\"fuel_type\", \"clean_title\", \"accident\", \"transmission\"]:\n            if column in self.train.columns:\n                unique_values = self.train.unique(subset=[column], maintain_order=True)[column]\n                pl_null = unique_values[2] if len(unique_values) > 2 else None  # Adjust index as needed\n                mapping = {'â€“': pl_null, \"not supported\": pl_null, pl_null: \"Unknown\"}\n                self.train = self.train.with_columns(pl.col(column).replace(mapping))\n                if self.test and column in self.test.columns:\n                    self.test = self.test.with_columns(pl.col(column).replace(mapping))\n                self.train = self.train.with_columns(pl.col(column).replace(pl_null, \"Unknown\"))\n                if self.test and column in self.test.columns:\n                    self.test = self.test.with_columns(pl.col(column).replace(pl_null, \"Unknown\"))\n        print(\"Null values replaced.\")\n\n    def categorize_transmission(self, trans):\n        if trans is None or pd.isnull(trans):\n            return \"Other\"\n        trans = trans.lower()\n        if \"manual\" in trans or \"m/t\" in trans:\n            return \"Manual\"\n        elif \"automatic\" in trans or \"a/t\" in trans or \"cvt\" in trans:\n            return \"Automatic\"\n        else:\n            return \"Other\"\n\n    def extract_speed(self, trans):\n        if trans is None or pd.isnull(trans):\n            return \"Other\"\n        match = re.search(r\"(\\d+)-speed\", trans, re.IGNORECASE)\n        if match:\n            return match.group(1)\n        else:\n            return \"Other\"\n\n    def categorize_transmissions(self):\n        # Apply categorization on training data\n        df_pandas = self.train.to_pandas()\n        df_pandas[\"transmission_category\"] = df_pandas[\"transmission\"].apply(self.categorize_transmission)\n        df_pandas[\"speed_category\"] = df_pandas[\"transmission\"].apply(self.extract_speed)\n        self.train = pl.from_pandas(df_pandas)\n\n        # Apply the same to testing data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            df_test[\"transmission_category\"] = df_test[\"transmission\"].apply(self.categorize_transmission)\n            df_test[\"speed_category\"] = df_test[\"transmission\"].apply(self.extract_speed)\n            self.test = pl.from_pandas(df_test)\n        print(\"Transmissions categorized.\")\n\n    def extract_engine_details(self, engine_str):\n        if pd.isnull(engine_str):\n            return 0.0, 0.0, 0\n        hp_match = re.search(r\"(\\d{2,3}\\.?\\d*)HP\", engine_str)\n        l_match = re.search(r\"(\\d\\.?\\d*)L\", engine_str)\n        cyl_match = re.search(r\"(\\d+) Cylinder\", engine_str)\n        hp = float(hp_match.group(1)) if hp_match else 0.0\n        liters = float(l_match.group(1)) if l_match else 0.0\n        cylinders = int(cyl_match.group(1)) if cyl_match else 0\n        return hp, liters, cylinders\n\n    def apply_engine_extraction(self):\n        # Apply extraction on training data\n        df_pandas = self.train.to_pandas()\n        engine_details = df_pandas['engine'].apply(self.extract_engine_details)\n        df_pandas['HP'], df_pandas['Liters'], df_pandas['Cylinders'] = zip(*engine_details)\n        self.train = pl.from_pandas(df_pandas)\n        self.train = self.train.drop([\"transmission\", \"engine\"])\n        \n        # Apply the same to testing data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            engine_details_test = df_test['engine'].apply(self.extract_engine_details)\n            df_test['HP'], df_test['Liters'], df_test['Cylinders'] = zip(*engine_details_test)\n            self.test = pl.from_pandas(df_test)\n            self.test = self.test.drop([\"transmission\", \"engine\"])\n        print(\"Engine details extracted.\")\n\n    def detect_categorical_columns(self):\n        \"\"\"\n        Automatically detect categorical columns based on data types and additional specifications.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        # Detect categorical columns based on data types\n        categorical_columns = df_pandas.select_dtypes(include=['object', 'category', 'bool', 'string']).columns.tolist()\n        \n        # Include additional categorical columns provided by the user\n        for col in self.additional_categorical:\n            if col in df_pandas.columns and col not in categorical_columns:\n                categorical_columns.append(col)\n        \n        print(f\"Detected categorical columns: {categorical_columns}\")\n        return categorical_columns\n\n    def label_encode_and_handle_rare(self, cat_cols):\n        \"\"\"\n        Label encode categorical columns and handle rare categories.\n        Rare categories are replaced with 0.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        if self.test:\n            df_test = self.test.to_pandas()\n        else:\n            df_test = None\n\n        for c in cat_cols:\n            print(f\"\\nProcessing categorical column: {c}\")\n            # Ensure the column is of a categorical type\n            if df_pandas[c].dtype not in ['object', 'category', 'bool', 'string', 'int64']:\n                print(f\"Skipping column {c} as it is not of a categorical type.\")\n                continue\n            # Factorize to get integer codes\n            df_pandas[c], uniques = pd.factorize(df_pandas[c], sort=True)\n            if df_test is not None:\n                # Apply the same factorization to test data\n                df_test[c] = pd.Categorical(df_test[c], categories=uniques).codes\n                df_test[c] = df_test[c].astype(int)\n            # Shift to ensure minimum label is 0\n            df_pandas[c] -= df_pandas[c].min()\n            if df_test is not None:\n                df_test[c] -= df_pandas[c].min()\n            # Get value counts\n            vc = df_pandas[c].value_counts()\n            # Identify rare categories\n            rare_categories = vc[vc < self.rare_threshold].index.values\n            self.RARE.append(rare_categories)\n            # Number of unique categories excluding rare\n            n_unique = df_pandas[c].nunique()\n            min_val = df_pandas[c].min()\n            max_val = df_pandas[c].max()\n            rare_count = len(rare_categories)\n            print(f'{c}: nunique={n_unique}, min={min_val}, max={max_val}, rare_ct={rare_count}')\n            # Update CAT_SIZE and CAT_EMB\n            # +1 for rare category\n            cat_size = max_val + 2  # Adding one more for rare\n            self.CAT_SIZE.append(cat_size)\n            self.CAT_EMB.append(int(np.ceil(np.sqrt(cat_size))))\n            # Increment labels by 1 to reserve 0 for rare\n            df_pandas[c] += 1\n            if df_test is not None:\n                df_test[c] += 1\n            # Replace rare categories with 0\n            rare_indices = rare_categories + 1  # Since labels have been incremented\n            df_pandas.loc[df_pandas[c].isin(rare_indices), c] = 0\n            if df_test is not None:\n                df_test.loc[df_test[c].isin(rare_indices), c] = 0\n            # Store LabelEncoder (if needed elsewhere)\n            le = LabelEncoder()\n            # Fit on non-rare categories\n            non_rare = df_pandas[df_pandas[c] != 0][c]\n            le.fit(non_rare)\n            self.label_encoders[c] = le\n            print(f\"Label encoding completed for column: {c}\")\n        \n        # Update the training and testing data\n        self.train = pl.from_pandas(df_pandas)\n        if self.test:\n            self.test = pl.from_pandas(df_test)\n        print(\"\\nLabel encoding and rare category handling complete.\")\n\n    def fill_nulls(self):\n        df_pandas = self.train.to_pandas()\n        numerical_cols = ['speed_category', 'HP', 'Liters', 'Cylinders']\n        for col in numerical_cols:\n            if col in df_pandas.columns:\n                df_pandas[col].fillna(0, inplace=True)\n                if self.test and col in self.test.columns:\n                    df_test = self.test.to_pandas()\n                    df_test[col].fillna(0, inplace=True)\n                    self.test = pl.from_pandas(df_test)\n        self.train = pl.from_pandas(df_pandas)\n        print(\"Null values filled.\")\n\n    def drop_columns(self, columns):\n        existing_columns_train = self.train.columns\n        columns_to_drop = [col for col in columns if col in existing_columns_train]\n        self.train = self.train.drop(columns_to_drop)\n        if self.test:\n            existing_columns_test = self.test.columns\n            columns_to_drop_test = [col for col in columns if col in existing_columns_test]\n            self.test = self.test.drop(columns_to_drop_test)\n        print(f\"Dropped columns: {columns_to_drop}\")\n\n    def detect_numerical_columns(self):\n        \"\"\"\n        Detect numerical columns that need to be scaled.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        numerical_columns = df_pandas.select_dtypes(include=['float64', 'int64']).columns.tolist()\n        # Exclude categorical columns\n        numerical_columns = [col for col in numerical_columns if col not in self.CATS]\n        # Further exclude target variable if present\n        if 'price' in numerical_columns:\n            numerical_columns.remove('price')\n        self.NUMS = numerical_columns\n        print(f\"Detected numerical columns for scaling: {self.NUMS}\")\n        return self.NUMS\n\n    def scale_features(self):\n        df_pandas = self.train.to_pandas()\n        numerical_columns = self.NUMS\n        print(f\"Scaling numerical columns: {numerical_columns}\")\n        # Fit scaler on training data\n        df_pandas[numerical_columns] = self.scaler.fit_transform(df_pandas[numerical_columns])\n        self.train = pl.from_pandas(df_pandas)\n        \n        # Apply scaler to test data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            df_test[numerical_columns] = self.scaler.transform(df_test[numerical_columns])\n            self.test = pl.from_pandas(df_test)\n        print(\"Features scaled.\")\n\n    def preprocess(self):\n        self.load_data()\n        self.replace_null_values()\n        self.categorize_transmissions()\n        self.apply_engine_extraction()\n        # Automatically detect categorical columns\n        categorical_columns = self.detect_categorical_columns()\n        self.CATS = categorical_columns\n        self.label_encode_and_handle_rare(cat_cols=categorical_columns)\n        self.fill_nulls()\n        self.drop_columns([\"id\"])  # Ensure 'id' exists or handle if missing\n        self.detect_numerical_columns()\n        self.scale_features()\n        print(\"Preprocessing complete.\")\n\n    def get_embedding_info(self):\n        \"\"\"\n        Returns a dictionary with categorical columns as keys and a tuple of (CAT_SIZE, CAT_EMB)\n        \"\"\"\n        embedding_info = {}\n        for idx, c in enumerate(self.CATS):\n            embedding_info[c] = (self.CAT_SIZE[idx], self.CAT_EMB[idx])\n        return embedding_info\n\n    def print_dataframe(self, dataset='train'):\n        # Display the dataframe\n        if dataset == 'train':\n            return self.train\n        elif dataset == 'test' and self.test is not None:\n            return self.test\n        else:\n            print(f\"No dataset named '{dataset}' found.\")\n            return None\n\n# Example usage:\n# preprocessor = DataPreprocessor(\n#     train_file='train.csv',\n#     test_file='test.csv',\n#     rare_threshold=40,\n#     additional_categorical=['brand', 'model']  # Specify additional categorical columns here\n# )\n# preprocessor.preprocess()\n# processed_train = preprocessor.print_dataframe('train')\n# processed_test = preprocessor.print_dataframe('test')\n# embedding_info = preprocessor.get_embedding_info()\n# print(embedding_info)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.641224Z","iopub.execute_input":"2024-09-15T03:58:22.641860Z","iopub.status.idle":"2024-09-15T03:58:22.659234Z","shell.execute_reply.started":"2024-09-15T03:58:22.641814Z","shell.execute_reply":"2024-09-15T03:58:22.657982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile plots.py\n\n\nimport os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport shap\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Plotter:\n    def __init__(self, X, y):\n        \"\"\"\n        Initializes the Plotter with preprocessed and scaled features and target data.\n        \n        Parameters:\n        - X: Feature DataFrame (already scaled)\n        - y: Target Series (already scaled)\n        \"\"\"\n        self.X = X\n        self.y = y\n\n\n    def plot_correlation_matrix(self):\n        \"\"\"\n        Plots the correlation matrix of the combined feature and target data.\n        \"\"\"\n        # Combine X and y into one DataFrame\n        df_combined = pd.concat([self.X, self.y], axis=1)\n\n        # Calculate the correlation matrix\n        corr_matrix = df_combined.corr()\n\n        # Plot the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n        plt.title(\"Correlation Matrix\", fontsize=16)\n        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels slightly for better readability\n        plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n        plt.tight_layout()\n        plt.show()\n        \n    \n    def plot_feature_importance(self, model):\n        \"\"\"\n        Plots feature importance using the provided trained model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor) with a `feature_importances_` attribute.\n        \"\"\"\n        feature_importances = model.feature_importances_\n        importance_df = pd.DataFrame({\n            'Feature': self.X.columns,\n            'Importance': feature_importances\n        }).sort_values(by='Importance', ascending=False)\n\n        # Plot Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n        plt.title(\"Feature Importance\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_permutation_importance(self, model, random_state=42, n_estimators=100, n_repeats=10):\n        \"\"\"\n        Plots permutation feature importance based on the preprocessed data (already scaled).\n        \n        Parameters:\n        - random_state: Random seed for reproducibility (default 42)\n        - n_estimators: Number of trees in the RandomForestRegressor (default 100)\n        - n_repeats: Number of times to shuffle the data during permutation importance (default 10)\n        \"\"\"\n\n        perm_importance = permutation_importance(model, self.X, self.y, n_repeats=n_repeats, random_state=random_state, scoring='neg_root_mean_squared_error')\n\n        # Step 3: Create a DataFrame to store the results\n        perm_importance_df = pd.DataFrame({\n            'Feature': self.X.columns,  # Use X's original column names\n            'Importance': perm_importance.importances_mean\n        }).sort_values(by='Importance', ascending=False)\n\n        # Step 4: Plot Permutation Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=perm_importance_df, palette='coolwarm')\n        plt.title(\"Permutation Feature Importance (Scaled)\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n        \n        \n    def plot_shap_summary(self, model):\n        \"\"\"\n        Plots a SHAP summary plot for the provided model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor)\n        \"\"\"\n        # Create SHAP explainer\n        explainer = shap.TreeExplainer(model)\n\n        # Calculate SHAP values for the feature set\n        shap_values = explainer.shap_values(self.X)\n\n        # Plot SHAP summary plot (global interpretation)\n        shap.summary_plot(shap_values, self.X, feature_names=self.X.columns)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.661017Z","iopub.execute_input":"2024-09-15T03:58:22.661479Z","iopub.status.idle":"2024-09-15T03:58:22.685060Z","shell.execute_reply.started":"2024-09-15T03:58:22.661430Z","shell.execute_reply":"2024-09-15T03:58:22.683793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import data_preprocessing\nimport plots\nimport importlib\nimportlib.reload(data_preprocessing)\nimportlib.reload(plots)\nfrom data_preprocessing import DataPreprocessor\nfrom plots import Plotter","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.687881Z","iopub.execute_input":"2024-09-15T03:58:22.688346Z","iopub.status.idle":"2024-09-15T03:58:22.711316Z","shell.execute_reply.started":"2024-09-15T03:58:22.688301Z","shell.execute_reply":"2024-09-15T03:58:22.709780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATAPATH = \"/kaggle/input/playground-series-s4e9/\"\nTRAIN_SET = DATAPATH + \"train.csv\"\nTEST_SET = DATAPATH + \"test.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.712969Z","iopub.execute_input":"2024-09-15T03:58:22.713443Z","iopub.status.idle":"2024-09-15T03:58:22.718927Z","shell.execute_reply.started":"2024-09-15T03:58:22.713396Z","shell.execute_reply":"2024-09-15T03:58:22.717757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = pl.read_csv(TRAIN_SET)\n# y = data[\"price\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.720422Z","iopub.execute_input":"2024-09-15T03:58:22.720840Z","iopub.status.idle":"2024-09-15T03:58:22.733912Z","shell.execute_reply.started":"2024-09-15T03:58:22.720790Z","shell.execute_reply":"2024-09-15T03:58:22.732551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # Example usage\n# preprocessor = DataPreprocessor(TRAIN_SET)\n# preprocessor.preprocess()\n# train = preprocessor.train.drop(\"price\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.735355Z","iopub.execute_input":"2024-09-15T03:58:22.735738Z","iopub.status.idle":"2024-09-15T03:58:22.753147Z","shell.execute_reply.started":"2024-09-15T03:58:22.735699Z","shell.execute_reply":"2024-09-15T03:58:22.751806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Install necessary packages\n# !pip install tensorflow deeptables colorama","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.754937Z","iopub.execute_input":"2024-09-15T03:58:22.755370Z","iopub.status.idle":"2024-09-15T03:58:22.770317Z","shell.execute_reply.started":"2024-09-15T03:58:22.755326Z","shell.execute_reply":"2024-09-15T03:58:22.768786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_SET","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.772147Z","iopub.execute_input":"2024-09-15T03:58:22.772701Z","iopub.status.idle":"2024-09-15T03:58:22.787507Z","shell.execute_reply.started":"2024-09-15T03:58:22.772619Z","shell.execute_reply":"2024-09-15T03:58:22.786198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport tensorflow.keras.backend as K\n\n# Import the DataPreprocessor class from your preprocessing script\nfrom data_preprocessing import DataPreprocessor\n\n# Define version for saving models and OOF\nVER = 1  # You can change this version number as needed\n\n# Initialize the DataPreprocessor with your dataset paths\npreprocessor = DataPreprocessor(\n    train_file=TRAIN_SET,   # Replace with your actual training file path\n    rare_threshold=40,\n    additional_categorical=['brand', 'model']  # Add any additional categorical columns if necessary\n)\n\n# Run the preprocessing\npreprocessor.preprocess()\n\n# Retrieve the preprocessed data as pandas DataFrames\ntrain = preprocessor.train.to_pandas()\n\nprint(\"Preprocessing of Training Set Complete\")\nprint(\"\\n\" + \"#\"*20 + \"\\n\")\n\npreprocessor_test = DataPreprocessor(\n    train_file=TEST_SET,   # Replace with your actual training file path\n#     test_file=TEST_SET,     # Replace with your actual testing file path\n    rare_threshold=40,\n    additional_categorical=['brand', 'model']  # Add any additional categorical columns if necessary\n)\n\npreprocessor_test.preprocess()\ntest = preprocessor_test.train.to_pandas()\n\nprint(\"Preprocessing of Testing Set Complete\")\nprint(\"\\n\" + \"#\"*20 + \"\\n\")\n\n# Handle unseen categories in the test set by setting them to 0\nCATS = preprocessor.CATS\nfor c in CATS:\n    # Get unique values from training and testing data\n    train_unique = train[c].unique()\n    test_unique = test[c].unique()\n    \n    # Find categories in test not present in train\n    unseen = np.setdiff1d(test_unique, train_unique)\n    print(f\"{c}: Test has label encodes = {unseen} which are not in train.\")\n    \n    if len(unseen) > 0:\n        num_unseen = test[test[c].isin(unseen)].shape[0]\n        print(f\" => {num_unseen} rows with unseen categories in test.\")\n        # Relabel unseen categories as 0\n        test.loc[test[c].isin(unseen), c] = 0\n\n# Retrieve numerical columns and embedding information\nNUMS = preprocessor.NUMS\nCAT_SIZE = preprocessor.CAT_SIZE\nCAT_EMB = preprocessor.CAT_EMB\n\n# Define the target variable\nTARGET = 'price'  # Replace with your actual target column name if different\n\n# Check if the target variable exists in the training data\nif TARGET not in train.columns:\n    raise ValueError(f\"Target column '{TARGET}' not found in the training data.\")\n\n# Set up K-Fold Cross Validation\nFOLDS = 5\nkf = KFold(n_splits=FOLDS, random_state=42, shuffle=True)\n\n# Initialize arrays for out-of-fold predictions and test predictions\noof = np.zeros(len(train))\npred = np.zeros(len(test))\n\n# Create directory for saving model checkpoints\ndirectory = \"checkpoints\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# Define the learning rate schedule\nEPOCHS = 5\nLRS = [0.001]*2 + [0.0001]*2 + [0.00001]*1  # Adjust the learning rates as needed\n\ndef lrfn(epoch):\n    return LRS[epoch] if epoch < len(LRS) else LRS[-1]\n\nrng = list(range(EPOCHS))\nlr_y = [lrfn(x) for x in rng]\n\n# Plot the learning rate schedule\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(lr_y[0], max(lr_y), lr_y[-1]))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Learning Rate Schedule\")\nplt.show()\n\n# Define the learning rate callback\nlr_callback = LearningRateScheduler(lrfn, verbose=True)\n\n# Function to build the TensorFlow model\ndef build_model():\n    # CATEGORICAL FEATURES\n    x_input_cats = Input(shape=(len(CATS),), name='categorical_inputs')\n    embs = []\n    for j in range(len(CATS)):\n        e = Embedding(input_dim=CAT_SIZE[j], output_dim=CAT_EMB[j], name=f'embedding_{CATS[j]}')(x_input_cats[:, j])\n        x = Flatten()(e)\n        embs.append(x)\n    \n    # NUMERICAL FEATURES\n    x_input_nums = Input(shape=(len(NUMS),), name='numerical_inputs')\n    \n    # COMBINE Categorical Embeddings and Numerical Inputs\n    x = Concatenate(axis=-1)(embs + [x_input_nums])\n    \n    # Dense Layers\n    x = Dense(256, activation='relu')(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(256, activation='relu')(x)\n    output = Dense(1, activation='linear')(x)\n    \n    model = Model(inputs=[x_input_cats, x_input_nums], outputs=output)\n    return model\n\n# Start K-Fold Cross Validation\nfor i, (train_idx, valid_idx) in enumerate(kf.split(train)):\n    print(\"#\"*25)\n    print(f\"### Fold {i+1} ###\")\n    print(\"#\"*25)\n    \n    # Split the data into training and validation sets for this fold\n    X_train_cats = train.iloc[train_idx][CATS].values\n    X_train_nums = train.iloc[train_idx][NUMS].values\n    y_train = train.iloc[train_idx][TARGET].values\n    \n    X_valid_cats = train.iloc[valid_idx][CATS].values\n    X_valid_nums = train.iloc[valid_idx][NUMS].values\n    y_valid = train.iloc[valid_idx][TARGET].values\n    \n    X_test_cats = test[CATS].values\n    X_test_nums = test[NUMS].values\n    \n    # Clear previous TensorFlow session\n    K.clear_session()\n    \n    # Build and compile the model\n    model = build_model()\n    initial_lr = 0.001  # Starting learning rate\n    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n    model.compile(optimizer=optimizer, \n                  loss=\"mean_squared_error\", \n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    # Train the model\n    history = model.fit(\n        [X_train_cats, X_train_nums], y_train, \n        validation_data=([X_valid_cats, X_valid_nums], y_valid),\n        callbacks=[lr_callback],\n        batch_size=64, \n        epochs=EPOCHS, \n        verbose=2\n    )\n    \n    # Save model weights\n    model.save_weights(f'{directory}/NN_v{VER}_f{i+1}.weights.h5')\n    \n    # Predict on the validation set\n    oof_preds = model.predict([X_valid_cats, X_valid_nums], verbose=1, batch_size=512).flatten()\n    rmse = np.sqrt(np.mean((oof_preds - y_valid) ** 2))\n    print(f' => RMSE for fold {i+1}: {rmse}\\n')\n    \n    # Store out-of-fold predictions\n    oof[valid_idx] = oof_preds\n    \n    # Predict on the test set\n    test_preds = model.predict([X_test_cats, X_test_nums], verbose=1, batch_size=512).flatten()\n    pred += test_preds\n\n# Average test predictions over all folds\npred /= FOLDS\n\n# Compute and display overall CV RMSE\noverall_rmse = np.sqrt(np.mean((oof - train[TARGET].values) ** 2))\nprint(\"\\n\\n\" + \"-#-#\" * 25 + \"\\n\\n\" + f\"Overall CV RMSE = {overall_rmse}\")\n\n# # Save OOF predictions to CSV\n# oof_df = train[['id']].copy()  # Ensure 'id' column exists\n# oof_df['pred'] = oof\n# oof_df.to_csv(f\"oof_v{VER}.csv\", index=False)\n# print(f\"OOF predictions saved to oof_v{VER}.csv\")\ntest_id = pd.read_csv(TEST_SET)[\"id\"]\n# # Optionally, save test predictions\n# test_predictions = pd.DataFrame({\n#     'id': test_id,  # Ensure 'id' column exists in test set\n#     'price': pred\n# })\n# test_predictions.to_csv(f\"test_predictions_v{VER}.csv\", index=False)\n# print(f\"Test predictions saved to test_predictions_v{VER}.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:58:22.793111Z","iopub.execute_input":"2024-09-15T03:58:22.793632Z","iopub.status.idle":"2024-09-15T04:04:43.027793Z","shell.execute_reply.started":"2024-09-15T03:58:22.793560Z","shell.execute_reply":"2024-09-15T04:04:43.024594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor.print_dataframe()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.028722Z","iopub.status.idle":"2024-09-15T04:04:43.029169Z","shell.execute_reply.started":"2024-09-15T04:04:43.028964Z","shell.execute_reply":"2024-09-15T04:04:43.028985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# import tensorflow as tf\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.layers import Dense, Dropout, Input, Embedding\n# from tensorflow.keras.layers import Concatenate, Multiply\n# import tensorflow.keras.backend as K\n\n# print('TF Version',tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.030931Z","iopub.status.idle":"2024-09-15T04:04:43.031625Z","shell.execute_reply.started":"2024-09-15T04:04:43.031353Z","shell.execute_reply":"2024-09-15T04:04:43.031385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EPOCHS = 3\n# LRS = [0.001]*2 + [0.0001]*1\n\n# def lrfn(epoch):\n#     return LRS[epoch]\n\n# rng = [i for i in range(EPOCHS)]\n# lr_y = [lrfn(x) for x in rng]\n# plt.figure(figsize=(10, 4))\n# plt.plot(rng, lr_y, '-o')\n# print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n#         format(lr_y[0], max(lr_y), lr_y[-1]))\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Learning Rate\")\n# plt.title(\"Learning Rate Schedule\")\n# plt.show()\n\n# lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.034282Z","iopub.status.idle":"2024-09-15T04:04:43.034754Z","shell.execute_reply.started":"2024-09-15T04:04:43.034550Z","shell.execute_reply":"2024-09-15T04:04:43.034571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# if gpus:\n#     try:\n#         for gpu in gpus:\n#             tf.config.experimental.set_memory_growth(gpu, True)\n#     except RuntimeError as e:\n#         print(e)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.037754Z","iopub.status.idle":"2024-09-15T04:04:43.038202Z","shell.execute_reply.started":"2024-09-15T04:04:43.037999Z","shell.execute_reply":"2024-09-15T04:04:43.038020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.040116Z","iopub.status.idle":"2024-09-15T04:04:43.040748Z","shell.execute_reply.started":"2024-09-15T04:04:43.040511Z","shell.execute_reply":"2024-09-15T04:04:43.040538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow deeptables","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.042403Z","iopub.status.idle":"2024-09-15T04:04:43.042864Z","shell.execute_reply.started":"2024-09-15T04:04:43.042663Z","shell.execute_reply":"2024-09-15T04:04:43.042684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np, pandas as pd\n# from colorama import Fore, Style\n# from sklearn.model_selection import KFold\n\n# import tensorflow as tf, deeptables as dt\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.utils import plot_model\n# from tensorflow.keras.optimizers.legacy import Adam\n# from deeptables.models import DeepTable, ModelConfig\n# from deeptables.models import deepnets","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.045768Z","iopub.status.idle":"2024-09-15T04:04:43.046334Z","shell.execute_reply.started":"2024-09-15T04:04:43.046062Z","shell.execute_reply":"2024-09-15T04:04:43.046088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EPOCHS = 3\n# LRS = [0.001]*2 + [0.0005]*1\n\n# def lrfn(epoch):\n#     return LRS[epoch]\n\n# rng = [i for i in range(EPOCHS)]\n# lr_y = [lrfn(x) for x in rng]\n# plt.figure(figsize=(10, 4))\n# plt.plot(rng, lr_y, '-o')\n# print(\"Learning rate schedule: {:.3g} to {:.3g}\". \\\n#       format(lr_y[0], lr_y[-1]))\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Learning Rate\")\n# plt.title(\"Learning Rate Schedule\")\n# plt.show()\n\n# lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn,\n#                                                        verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.048375Z","iopub.status.idle":"2024-09-15T04:04:43.048871Z","shell.execute_reply.started":"2024-09-15T04:04:43.048654Z","shell.execute_reply":"2024-09-15T04:04:43.048680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.optimizers import Adam\n# # import deepnets  # Ensure this is updated to a Keras 3-compatible version\n\n# class CFG:\n#     folds = 5\n#     epochs = 3\n#     batch_size = 32\n#     LR_Scheduler = [lr_callback]  # Define lr_callback appropriately\n#     optimizer = Adam(learning_rate=1e-3)\n\n# conf = ModelConfig(\n#     auto_imputation=False,\n#     auto_discrete=True,\n#     auto_discard_unique=False,\n#     categorical_columns='auto',\n#     fixed_embedding_dim=True,\n#     embeddings_output_dim=7,\n#     embedding_dropout=0.3,\n#     nets=deepnets.DeepFM + ['fg_nets'],\n#     dnn_params={\n#         'hidden_units': (\n#             (768, 0.3, True),\n#             (768, 0.3, True)\n#         ),\n#         'dnn_activation': 'relu',\n#     },\n#     stacking_op='concat',\n#     output_use_bias=False,\n#     optimizer=CFG.optimizer,\n#     task='regression',\n#     loss='auto',\n#     metrics=[\"RootMeanSquaredError\"],\n#     earlystopping_patience=1,\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.050812Z","iopub.status.idle":"2024-09-15T04:04:43.051421Z","shell.execute_reply.started":"2024-09-15T04:04:43.051066Z","shell.execute_reply":"2024-09-15T04:04:43.051090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.053841Z","iopub.status.idle":"2024-09-15T04:04:43.054790Z","shell.execute_reply.started":"2024-09-15T04:04:43.054188Z","shell.execute_reply":"2024-09-15T04:04:43.054505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# kf = KFold(n_splits=CFG.folds, random_state=42, shuffle=True)\n\n# oof = np.zeros(len(train))\n# pred = np.zeros(len(test))\n\n# for i, (train_index, test_index) in enumerate(kf.split(train)):\n#     X_train = train.loc[train_index,CATS+NUMS]\n#     y_train = train.loc[train_index,\"price\"]\n    \n#     X_valid = train.loc[test_index,CATS+NUMS]\n#     y_valid = train.loc[test_index,\"price\"]\n    \n#     X_test = test[CATS+NUMS]\n    \n#     print(\"#\"*25)\n#     print(f\"### Fold {i+1} ###\")\n#     print(\"#\"*25)\n    \n#     # TRAIN MODEL\n#     K.clear_session()\n#     model = DeepTable(config=conf)\n#     model.fit(X_train, y_train, \n#               validation_data=(X_valid, y_valid),\n#               callbacks=CFG.LR_Scheduler,\n#               batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n    \n#     # Avoid saving error\n#     with K.name_scope(CFG.optimizer.__class__.__name__):\n#         for j, var in enumerate(CFG.optimizer.weights):\n#             name = 'variable{}'.format(j)\n#             CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n#     conf = conf._replace(optimizer=CFG.optimizer)\n    \n#     # OOF PREDS\n#     oof_preds = model.predict(X_valid, verbose=1, batch_size=512).flatten()\n#     rmse = np.round(np.sqrt(np.mean((oof_preds - y_valid)**2)),4)\n#     print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {i+1} | rmse: {rmse}\\n')\n#     if i<CFG.folds: oof[test_index] = oof_preds\n#     else: oof[test_index] += oof_preds\n        \n#     # TEST PREDS\n#     test_preds = model.predict(X_test, verbose=1, batch_size=512).flatten()\n#     if i==0: pred = test_preds\n#     else: pred += test_preds\n    \n# pred /= CFG.folds\n# plot_model(model.get_model().model)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.056524Z","iopub.status.idle":"2024-09-15T04:04:43.056992Z","shell.execute_reply.started":"2024-09-15T04:04:43.056772Z","shell.execute_reply":"2024-09-15T04:04:43.056792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rmse = np.round(np.sqrt(np.mean((oof - train.price.values)**2)),4)\n# print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}')\n\n# # SAVE OOF \n# oof_df = train[[\"id\"]].copy()\n# oof_df[\"pred\"] = oof\n# oof_df.to_csv(\"oof.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.058636Z","iopub.status.idle":"2024-09-15T04:04:43.059739Z","shell.execute_reply.started":"2024-09-15T04:04:43.059454Z","shell.execute_reply":"2024-09-15T04:04:43.059493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.063302Z","iopub.status.idle":"2024-09-15T04:04:43.063803Z","shell.execute_reply.started":"2024-09-15T04:04:43.063591Z","shell.execute_reply":"2024-09-15T04:04:43.063614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Dictionary for RandomForestRegressor parameters\n# rf_params = {\n#     'n_estimators': 100,\n#     'criterion': 'squared_error',\n#     'max_depth': None,\n#     'min_samples_split': 2,\n#     'min_samples_leaf': 1,\n#     'min_weight_fraction_leaf': 0.0,\n#     'max_features': 'auto',\n#     'max_leaf_nodes': None,\n#     'bootstrap': True,\n#     'oob_score': False,\n#     'n_jobs': -1,\n#     'random_state': 42,\n#     'verbose': 0,\n#     'warm_start': False,\n#     'ccp_alpha': 0.0,\n#     'max_samples': None\n# }\n\n\n\n# # Dictionary for SVR parameters\n# svr_params = {\n#     'kernel': 'rbf',\n#     'degree': 3,\n#     'gamma': 'scale',\n#     'coef0': 0.0,\n#     'tol': 1e-3,\n#     'C': 1.0,\n#     'epsilon': 0.1,\n#     'shrinking': True,\n#     'cache_size': 200,\n#     'verbose': False,\n#     'max_iter': -1\n# }\n\n\n# xgb_model = xgb.XGBRegressor(\n#     objective='reg:squarederror',\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     random_state=42,\n#     tree_method='gpu_hist'  # Enable GPU\n# )\n\n# lgb_model = lgb.LGBMRegressor(\n#     boosting_type='gbdt',\n#     num_leaves=31,\n#     learning_rate=0.1,\n#     n_estimators=100,\n#     random_state=42,\n#     device='gpu'  # Enable GPU\n# )\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.065057Z","iopub.status.idle":"2024-09-15T04:04:43.065533Z","shell.execute_reply.started":"2024-09-15T04:04:43.065310Z","shell.execute_reply":"2024-09-15T04:04:43.065334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# models_dict = {}","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.067980Z","iopub.status.idle":"2024-09-15T04:04:43.068450Z","shell.execute_reply.started":"2024-09-15T04:04:43.068221Z","shell.execute_reply":"2024-09-15T04:04:43.068257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Instantiate the RandomForestRegressor model using the parameters from the dictionary\n# rf_model = RandomForestRegressor(**rf_params)\n# rf_model.fit(X_train, y_train)\n# rf_predictions = rf_model.predict(X_test)\n# rf_mse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n# models_dict[rf_mse] = rf_model\n# print(f\"RandomForest RMSE: {rf_mse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.069411Z","iopub.status.idle":"2024-09-15T04:04:43.069892Z","shell.execute_reply.started":"2024-09-15T04:04:43.069692Z","shell.execute_reply":"2024-09-15T04:04:43.069715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Instantiate the SVR model using the parameters from the dictionary\n# svr_model = SVR(**svr_params)\n# svr_model.fit(X_train, y_train)\n# svr_predictions = svr_model.predict(X_test)\n# svr_mse = np.sqrt(mean_squared_error(y_test, svr_predictions))\n# models_dict[svr_mse] = svr_model\n# print(f\"SVR RMSE: {svr_mse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.071211Z","iopub.status.idle":"2024-09-15T04:04:43.071788Z","shell.execute_reply.started":"2024-09-15T04:04:43.071575Z","shell.execute_reply":"2024-09-15T04:04:43.071597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_model.fit(X_train, y_train)\n# xgb_predictions = xgb_model.predict(X_test)\n# xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n# models_dict[xgb_rmse] = xgb_model  # Store the RMSE and model\n# print(f\"XGBoost RMSE: {xgb_rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.074136Z","iopub.status.idle":"2024-09-15T04:04:43.074634Z","shell.execute_reply.started":"2024-09-15T04:04:43.074420Z","shell.execute_reply":"2024-09-15T04:04:43.074445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_model.fit(X_train, y_train)\n# lgb_predictions = lgb_model.predict(X_test)\n# lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_predictions))\n# models_dict[lgb_rmse] = lgb_model  # Store the RMSE and model\n# print(f\"LightGBM RMSE: {lgb_rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.076193Z","iopub.status.idle":"2024-09-15T04:04:43.076714Z","shell.execute_reply.started":"2024-09-15T04:04:43.076477Z","shell.execute_reply":"2024-09-15T04:04:43.076505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize Preprocessor for Test Data\n# preprocessor_test = DataPreprocessor(\n#     train_file=TEST_SET,  # Replace with your actual test file path\n#     rare_threshold=40,\n#     additional_categorical=['brand', 'model']\n# )\n# preprocessor_test.preprocess()\n# print(\"Test Data Preprocessing complete\")\n# test_data = preprocessor_test.train.to_pandas()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.078795Z","iopub.status.idle":"2024-09-15T04:04:43.079282Z","shell.execute_reply.started":"2024-09-15T04:04:43.079030Z","shell.execute_reply":"2024-09-15T04:04:43.079051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef create_submission(test_set, test_ids, predictions, output_file=\"submission.csv\"):\n    \"\"\"\n    Generate submission file using the accumulated test predictions.\n    \n    Parameters:\n    - test_set (pd.DataFrame): Preprocessed test set.\n    - test_ids (pd.Series or np.ndarray): Array of 'id's from the original test set.\n    - predictions (np.ndarray): Array of predicted 'price' values.\n    - output_file (str): Path to save the submission CSV.\n    \"\"\"\n    if len(test_ids) != len(predictions):\n        raise ValueError(\"Length of test_ids and predictions must be the same.\")\n    \n    # Create submission DataFrame with 'id' and 'price'\n    submission = pd.DataFrame({\n        'id': test_ids,\n        'price': predictions\n    })\n    \n    # Write to CSV\n    submission.to_csv(output_file, index=False)\n    print(f\"Submission file saved as {output_file}\")\n\n# Usage Example:\n# Assuming 'pred' contains your test predictions and 'test_id' contains the corresponding IDs\ncreate_submission(\n    test_set=test,\n    test_ids=test_id,\n    predictions=pred,\n    output_file=\"submission.csv\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.082405Z","iopub.status.idle":"2024-09-15T04:04:43.082942Z","shell.execute_reply.started":"2024-09-15T04:04:43.082708Z","shell.execute_reply":"2024-09-15T04:04:43.082732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Assuming 'model' is your trained Keras model and 'CATS' & 'NUMS' are defined\n# create_submission(\n#     test_set=test,\n#     model=model,\n#     CATS=CATS,\n#     NUMS=NUMS,\n#     TEST_SET=TEST_SET,  # Path to your original test CSV\n#     output_file=\"submission.csv\"\n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.084430Z","iopub.status.idle":"2024-09-15T04:04:43.084989Z","shell.execute_reply.started":"2024-09-15T04:04:43.084767Z","shell.execute_reply":"2024-09-15T04:04:43.084790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotter = Plotter(X, y)\n# plotter.plot_feature_importance(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.087614Z","iopub.status.idle":"2024-09-15T04:04:43.088090Z","shell.execute_reply.started":"2024-09-15T04:04:43.087870Z","shell.execute_reply":"2024-09-15T04:04:43.087895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### imports","metadata":{}},{"cell_type":"code","source":"# import data_preprocessing\n# import plots\n# import importlib\n# importlib.reload(data_preprocessing)\n# importlib.reload(plots)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.089667Z","iopub.status.idle":"2024-09-15T04:04:43.090121Z","shell.execute_reply.started":"2024-09-15T04:04:43.089916Z","shell.execute_reply":"2024-09-15T04:04:43.089938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile data_preprocessing.py\n\n# import os\n# import re\n# import polars as pl\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n# from sklearn.inspection import permutation_importance\n# import shap\n# import warnings\n\n# # Ignore all warnings\n# warnings.filterwarnings('ignore')\n\n\n# class DataPreprocessor:\n#     def __init__(self, train_file, test_file=None):\n#         self.train_file = train_file\n#         self.test_file = test_file\n#         self.data = None\n#         self.label_encoders = {}\n#         self.scaler = StandardScaler()\n\n#     def load_data(self):\n#         self.data = pl.read_csv(self.train_file)\n#         print(\"Data loaded successfully.\")\n    \n#     def replace_null_values(self):\n#         types = self.data.unique(subset=[\"fuel_type\"], maintain_order=True)[\"fuel_type\"]\n#         pl_null = types[2]\n#         mapping = {'â€“': pl_null, \"not supported\": pl_null, pl_null: \"Unknown\"}\n#         self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(mapping))\n#         self.data = self.data.with_columns(clean_title=pl.col(\"clean_title\").replace(mapping))\n#         self.data = self.data.with_columns(accident=pl.col(\"accident\").replace(mapping))\n#         self.data = self.data.with_columns(transmission=pl.col(\"transmission\").replace(mapping))\n#         self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(pl_null, \"Unknown\"))\n#         print(\"Null values replaced.\")\n    \n#     def categorize_transmission(self, trans):\n#         if trans is None:\n#             return None\n#         trans = trans.lower()\n#         if \"manual\" in trans or \"m/t\" in trans:\n#             return \"Manual\"\n#         elif \"automatic\" in trans or \"a/t\" in trans or \"cvt\" in trans:\n#             return \"Automatic\"\n#         else:\n#             return \"Other\"\n    \n#     def extract_speed(self, trans):\n#         if trans is None:\n#             return \"Other\"\n#         match = re.search(r\"(\\d+)-speed\", trans, re.IGNORECASE)\n#         if match:\n#             return match.group(1)\n#         else:\n#             return \"Other\"\n    \n#     def categorize_transmissions(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas[\"transmission_category\"] = df_pandas[\"transmission\"].apply(self.categorize_transmission)\n#         df_pandas[\"speed_category\"] = df_pandas[\"transmission\"].apply(self.extract_speed)\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Transmissions categorized.\")\n    \n#     def extract_engine_details(self, engine_str):\n#         if pd.isnull(engine_str):\n#             return None, None, None\n#         hp_match = re.search(r\"(\\d{2,3}\\.?\\d*)HP\", engine_str)\n#         l_match = re.search(r\"(\\d\\.?\\d*)L\", engine_str)\n#         cyl_match = re.search(r\"(\\d+) Cylinder\", engine_str)\n#         hp = hp_match.group(1) if hp_match else None\n#         liters = l_match.group(1) if l_match else None\n#         cylinders = cyl_match.group(1) if cyl_match else None\n#         return hp, liters, cylinders\n\n#     def apply_engine_extraction(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas['HP'], df_pandas['Liters'], df_pandas['Cylinders'] = zip(*df_pandas['engine'].apply(self.extract_engine_details))\n#         self.data = pl.from_pandas(df_pandas)\n#         self.data = self.data.drop(\"transmission\")\n#         self.data = self.data.drop(\"engine\")\n#         print(\"Engine details extracted.\")\n\n#     def label_encode(self):\n#         cat_data = self.data.select([col for col in self.data.columns if not self.data[col].dtype.is_numeric()])\n#         cat_data = cat_data.to_pandas()\n#         for column in cat_data.select_dtypes(include=['object']).columns:\n#             self.label_encoders[column] = LabelEncoder()\n#             cat_data[column] = self.label_encoders[column].fit_transform(cat_data[column])\n#         num_data = self.data.select([col for col in self.data.columns if self.data[col].dtype.is_numeric()])\n#         self.data = pl.from_pandas(pd.concat([num_data.to_pandas(), cat_data], axis=1))\n#         print(\"Categorical columns label encoded.\")\n\n#     def fill_nulls(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas['speed_category'].fillna(0, inplace=True)\n#         df_pandas['HP'].fillna(0, inplace=True)\n#         df_pandas['Liters'].fillna(0, inplace=True)\n#         df_pandas['Cylinders'].fillna(0, inplace=True)\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Null values filled.\")\n    \n#     def drop_columns(self, columns):\n#         self.data = self.data.drop(columns)\n#         print(f\"Dropped columns: {columns}\")\n\n#     def scale_features(self):\n#         df_pandas = self.data.to_pandas()\n#         numeric_columns = df_pandas.select_dtypes(include=['float64', 'int64']).columns\n#         df_pandas[numeric_columns] = self.scaler.fit_transform(df_pandas[numeric_columns])\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Features scaled.\")\n    \n#     def plot_feature_importance(self, target_column):\n#         X = self.data.drop(target_column).to_pandas()\n#         y = self.data[target_column].to_pandas()\n#         model = RandomForestRegressor(n_estimators=100, random_state=42)\n#         model.fit(X, y)\n#         feature_importances = model.feature_importances_\n#         importance_df = pd.DataFrame({\n#             'Feature': X.columns,\n#             'Importance': feature_importances\n#         }).sort_values(by='Importance', ascending=False)\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n#         plt.title(f\"Feature Importance with respect to '{target_column}'\")\n#         plt.tight_layout()\n#         plt.show()\n\n#     def preprocess(self):\n#         self.load_data()\n#         self.replace_null_values()\n#         self.categorize_transmissions()\n#         self.apply_engine_extraction()\n#         self.label_encode()\n#         self.fill_nulls()\n#         self.drop_columns([\"id\"])\n#         self.scale_features()\n#         print(\"Preprocessing complete.\")\n    \n#     def print_dataframe(self):\n#         # Display the dataframe\n#         return self.data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-15T04:04:43.091964Z","iopub.status.idle":"2024-09-15T04:04:43.092476Z","shell.execute_reply.started":"2024-09-15T04:04:43.092215Z","shell.execute_reply":"2024-09-15T04:04:43.092256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile plots.py\n\n\n# import os\n# import re\n# import polars as pl\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n# from sklearn.inspection import permutation_importance\n# import shap\n# import warnings\n\n# # Ignore all warnings\n# warnings.filterwarnings('ignore')\n\n\n# class Plotter:\n#     def __init__(self, X, y):\n#         \"\"\"\n#         Initializes the Plotter with preprocessed and scaled features and target data.\n        \n#         Parameters:\n#         - X: Feature DataFrame (already scaled)\n#         - y: Target Series (already scaled)\n#         \"\"\"\n#         self.X = X\n#         self.y = y\n\n\n#     def plot_correlation_matrix(self):\n#         \"\"\"\n#         Plots the correlation matrix of the combined feature and target data.\n#         \"\"\"\n#         # Combine X and y into one DataFrame\n#         df_combined = pd.concat([self.X, self.y], axis=1)\n\n#         # Calculate the correlation matrix\n#         corr_matrix = df_combined.corr()\n\n#         # Plot the correlation matrix using a heatmap\n#         plt.figure(figsize=(10, 8))\n#         sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n#         plt.title(\"Correlation Matrix\", fontsize=16)\n#         plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels slightly for better readability\n#         plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n#         plt.tight_layout()\n#         plt.show()\n        \n    \n#     def plot_feature_importance(self, model):\n#         \"\"\"\n#         Plots feature importance using the provided trained model.\n        \n#         Parameters:\n#         - model: A trained model (e.g., RandomForestRegressor) with a `feature_importances_` attribute.\n#         \"\"\"\n#         feature_importances = model.feature_importances_\n#         importance_df = pd.DataFrame({\n#             'Feature': self.X.columns,\n#             'Importance': feature_importances\n#         }).sort_values(by='Importance', ascending=False)\n\n#         # Plot Feature Importance\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n#         plt.title(\"Feature Importance\", fontsize=16)\n#         plt.tight_layout()\n#         plt.show()\n    \n#     def plot_permutation_importance(self, model, random_state=42, n_estimators=100, n_repeats=10):\n#         \"\"\"\n#         Plots permutation feature importance based on the preprocessed data (already scaled).\n        \n#         Parameters:\n#         - random_state: Random seed for reproducibility (default 42)\n#         - n_estimators: Number of trees in the RandomForestRegressor (default 100)\n#         - n_repeats: Number of times to shuffle the data during permutation importance (default 10)\n#         \"\"\"\n\n#         perm_importance = permutation_importance(model, self.X, self.y, n_repeats=n_repeats, random_state=random_state, scoring='neg_root_mean_squared_error')\n\n#         # Step 3: Create a DataFrame to store the results\n#         perm_importance_df = pd.DataFrame({\n#             'Feature': self.X.columns,  # Use X's original column names\n#             'Importance': perm_importance.importances_mean\n#         }).sort_values(by='Importance', ascending=False)\n\n#         # Step 4: Plot Permutation Feature Importance\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=perm_importance_df, palette='coolwarm')\n#         plt.title(\"Permutation Feature Importance (Scaled)\", fontsize=16)\n#         plt.tight_layout()\n#         plt.show()\n        \n        \n#     def plot_shap_summary(self, model):\n#         \"\"\"\n#         Plots a SHAP summary plot for the provided model.\n        \n#         Parameters:\n#         - model: A trained model (e.g., RandomForestRegressor)\n#         \"\"\"\n#         # Create SHAP explainer\n#         explainer = shap.TreeExplainer(model)\n\n#         # Calculate SHAP values for the feature set\n#         shap_values = explainer.shap_values(self.X)\n\n#         # Plot SHAP summary plot (global interpretation)\n#         shap.summary_plot(shap_values, self.X, feature_names=self.X.columns)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:04:43.094482Z","iopub.status.idle":"2024-09-15T04:04:43.094932Z","shell.execute_reply.started":"2024-09-15T04:04:43.094736Z","shell.execute_reply":"2024-09-15T04:04:43.094758Z"},"trusted":true},"execution_count":null,"outputs":[]}]}