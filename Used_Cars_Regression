{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport polars as pl\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATAPATH = \"/kaggle/input/playground-series-s4e9/\"\nTRAIN_SET = DATAPATH + \"train.csv\"\nTEST_SET = DATAPATH + \"test.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pl.read_csv(TRAIN_SET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport polars as pl\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile data_preprocessing.py\n\nimport os\nimport re\nimport pandas as pd\nimport polars as pl\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass DataPreprocessor:\n    def __init__(self, train_file, test_file=None):\n        self.train_file = train_file\n        self.test_file = test_file\n        self.data = None\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n\n    def load_data(self):\n        self.data = pl.read_csv(self.train_file)\n        print(\"Data loaded successfully.\")\n    \n    def replace_null_values(self):\n        types = self.data.unique(subset=[\"fuel_type\"], maintain_order=True)[\"fuel_type\"]\n        pl_null = types[2]\n        mapping = {'â€“': pl_null, \"not supported\": pl_null, pl_null: \"Unknown\"}\n        self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(mapping))\n        self.data = self.data.with_columns(clean_title=pl.col(\"clean_title\").replace(mapping))\n        self.data = self.data.with_columns(accident=pl.col(\"accident\").replace(mapping))\n        self.data = self.data.with_columns(transmission=pl.col(\"transmission\").replace(mapping))\n        self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(pl_null, \"Unknown\"))\n        print(\"Null values replaced.\")\n    \n    def categorize_transmission(self, trans):\n        if trans is None:\n            return None\n        trans = trans.lower()\n        if \"manual\" in trans or \"m/t\" in trans:\n            return \"Manual\"\n        elif \"automatic\" in trans or \"a/t\" in trans or \"cvt\" in trans:\n            return \"Automatic\"\n        else:\n            return \"Other\"\n    \n    def extract_speed(self, trans):\n        if trans is None:\n            return \"Other\"\n        match = re.search(r\"(\\d+)-speed\", trans, re.IGNORECASE)\n        if match:\n            return match.group(1)\n        else:\n            return \"Other\"\n    \n    def categorize_transmissions(self):\n        df_pandas = self.data.to_pandas()\n        df_pandas[\"transmission_category\"] = df_pandas[\"transmission\"].apply(self.categorize_transmission)\n        df_pandas[\"speed_category\"] = df_pandas[\"transmission\"].apply(self.extract_speed)\n        self.data = pl.from_pandas(df_pandas)\n        print(\"Transmissions categorized.\")\n    \n    def extract_engine_details(self, engine_str):\n        if pd.isnull(engine_str):\n            return None, None, None\n        hp_match = re.search(r\"(\\d{2,3}\\.?\\d*)HP\", engine_str)\n        l_match = re.search(r\"(\\d\\.?\\d*)L\", engine_str)\n        cyl_match = re.search(r\"(\\d+) Cylinder\", engine_str)\n        hp = hp_match.group(1) if hp_match else None\n        liters = l_match.group(1) if l_match else None\n        cylinders = cyl_match.group(1) if cyl_match else None\n        return hp, liters, cylinders\n\n    def apply_engine_extraction(self):\n        df_pandas = self.data.to_pandas()\n        df_pandas['HP'], df_pandas['Liters'], df_pandas['Cylinders'] = zip(*df_pandas['engine'].apply(self.extract_engine_details))\n        self.data = pl.from_pandas(df_pandas)\n        self.data = self.data.drop(\"transmission\")\n        self.data = self.data.drop(\"engine\")\n        print(\"Engine details extracted.\")\n\n    def label_encode(self):\n        cat_data = self.data.select([col for col in self.data.columns if not self.data[col].dtype.is_numeric()])\n        cat_data = cat_data.to_pandas()\n        for column in cat_data.select_dtypes(include=['object']).columns:\n            self.label_encoders[column] = LabelEncoder()\n            cat_data[column] = self.label_encoders[column].fit_transform(cat_data[column])\n        num_data = self.data.select([col for col in self.data.columns if self.data[col].dtype.is_numeric()])\n        self.data = pl.from_pandas(pd.concat([num_data.to_pandas(), cat_data], axis=1))\n        print(\"Categorical columns label encoded.\")\n\n    def fill_nulls(self):\n        df_pandas = self.data.to_pandas()\n        df_pandas['speed_category'].fillna(0, inplace=True)\n        df_pandas['HP'].fillna(0, inplace=True)\n        df_pandas['Liters'].fillna(0, inplace=True)\n        df_pandas['Cylinders'].fillna(0, inplace=True)\n        self.data = pl.from_pandas(df_pandas)\n        print(\"Null values filled.\")\n    \n    def drop_columns(self, columns):\n        self.data = self.data.drop(columns)\n        print(f\"Dropped columns: {columns}\")\n\n    def scale_features(self):\n        df_pandas = self.data.to_pandas()\n        numeric_columns = df_pandas.select_dtypes(include=['float64', 'int64']).columns\n        df_pandas[numeric_columns] = self.scaler.fit_transform(df_pandas[numeric_columns])\n        self.data = pl.from_pandas(df_pandas)\n        print(\"Features scaled.\")\n    \n    def plot_feature_importance(self, target_column):\n        X = self.data.drop(target_column).to_pandas()\n        y = self.data[target_column].to_pandas()\n        model = RandomForestRegressor(n_estimators=100, random_state=42)\n        model.fit(X, y)\n        feature_importances = model.feature_importances_\n        importance_df = pd.DataFrame({\n            'Feature': X.columns,\n            'Importance': feature_importances\n        }).sort_values(by='Importance', ascending=False)\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n        plt.title(f\"Feature Importance with respect to '{target_column}'\")\n        plt.tight_layout()\n        plt.show()\n\n    def preprocess(self):\n        self.load_data()\n        self.replace_null_values()\n        self.categorize_transmissions()\n        self.apply_engine_extraction()\n        self.label_encode()\n        self.fill_nulls()\n        self.drop_columns([\"id\"])\n        self.scale_features()\n        print(\"Preprocessing complete.\")\n    \n    def print_dataframe(self):\n        # Display the dataframe\n        return self.data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nfrom data_preprocessing import DataPreprocessor\npreprocessor = DataPreprocessor(TRAIN_SET, TEST_SET)\npreprocessor.preprocess()\n# preprocessor.plot_feature_importance(target_column=\"price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor.print_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = preprocessor.data.drop(\"price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data[\"price\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile plots.py\n\nclass Plotter:\n    def __init__(self, X, y):\n        \"\"\"\n        Initializes the Plotter with preprocessed and scaled features and target data.\n        \n        Parameters:\n        - X: Feature DataFrame (already scaled)\n        - y: Target Series (already scaled)\n        \"\"\"\n        self.X = X\n        self.y = y\n\n\n    def plot_correlation_matrix(self):\n        \"\"\"\n        Plots the correlation matrix of the combined feature and target data.\n        \"\"\"\n        # Combine X and y into one DataFrame\n        df_combined = pd.concat([self.X, self.y], axis=1)\n\n        # Calculate the correlation matrix\n        corr_matrix = df_combined.corr()\n\n        # Plot the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n        plt.title(\"Correlation Matrix\", fontsize=16)\n        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels slightly for better readability\n        plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n        plt.tight_layout()\n        plt.show()\n        \n    \n    def plot_feature_importance(self, model):\n        \"\"\"\n        Plots feature importance using the provided trained model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor) with a `feature_importances_` attribute.\n        \"\"\"\n        feature_importances = model.feature_importances_\n        importance_df = pd.DataFrame({\n            'Feature': self.X.columns,\n            'Importance': feature_importances\n        }).sort_values(by='Importance', ascending=False)\n\n        # Plot Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n        plt.title(\"Feature Importance\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_permutation_importance(self, model, random_state=42, n_estimators=100, n_repeats=10):\n        \"\"\"\n        Plots permutation feature importance based on the preprocessed data (already scaled).\n        \n        Parameters:\n        - random_state: Random seed for reproducibility (default 42)\n        - n_estimators: Number of trees in the RandomForestRegressor (default 100)\n        - n_repeats: Number of times to shuffle the data during permutation importance (default 10)\n        \"\"\"\n\n        perm_importance = permutation_importance(model, self.X, self.y, n_repeats=n_repeats, random_state=random_state, scoring='neg_root_mean_squared_error')\n\n        # Step 3: Create a DataFrame to store the results\n        perm_importance_df = pd.DataFrame({\n            'Feature': self.X.columns,  # Use X's original column names\n            'Importance': perm_importance.importances_mean\n        }).sort_values(by='Importance', ascending=False)\n\n        # Step 4: Plot Permutation Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=perm_importance_df, palette='coolwarm')\n        plt.title(\"Permutation Feature Importance (Scaled)\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n        \n        \n    def plot_shap_summary(self, model):\n        \"\"\"\n        Plots a SHAP summary plot for the provided model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor)\n        \"\"\"\n        # Create SHAP explainer\n        explainer = shap.TreeExplainer(model)\n\n        # Calculate SHAP values for the feature set\n        shap_values = explainer.shap_values(self.X)\n\n        # Plot SHAP summary plot (global interpretation)\n        shap.summary_plot(shap_values, self.X, feature_names=self.X.columns)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\n# Assuming X and y are preprocessed and scaled feature DataFrame and target Series:\nplotter = Plotter(X, y)\n\n# For feature importance, pass a trained model:\nplotter.plot_feature_importance(model)\n\n# For permutation importance:\nplotter.plot_permutation_importance(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\n\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n\n# Step 1: Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}