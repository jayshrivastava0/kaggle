{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T08:07:29.706588Z","iopub.execute_input":"2024-09-14T08:07:29.707238Z","iopub.status.idle":"2024-09-14T08:07:30.071002Z","shell.execute_reply.started":"2024-09-14T08:07:29.707204Z","shell.execute_reply":"2024-09-14T08:07:30.070052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e9/sample_submission.csv\n/kaggle/input/playground-series-s4e9/train.csv\n/kaggle/input/playground-series-s4e9/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport shap\nimport warnings\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nimport lightgbm as lgb\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T08:07:30.072678Z","iopub.execute_input":"2024-09-14T08:07:30.073052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile data_preprocessing.py\n\nimport os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nimport numpy as np\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\nclass DataPreprocessor:\n    def __init__(self, train_file, test_file=None, rare_threshold=40, additional_categorical=None):\n        \"\"\"\n        Initialize the DataPreprocessor.\n\n        Parameters:\n        - train_file (str): Path to the training CSV file.\n        - test_file (str, optional): Path to the testing CSV file.\n        - rare_threshold (int): Threshold below which categories are considered rare.\n        - additional_categorical (list, optional): List of additional categorical column names.\n        \"\"\"\n        self.train_file = train_file\n        self.test_file = test_file\n        self.train = None\n        self.test = None\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n        self.rare_threshold = rare_threshold\n        self.CAT_SIZE = []\n        self.CAT_EMB = []\n        self.RARE = []\n        self.CATS = []  # List of categorical columns to encode\n        self.NUMS = []  # List of numerical columns to scale\n        self.additional_categorical = additional_categorical if additional_categorical else []\n\n    def load_data(self):\n        self.train = pl.read_csv(self.train_file)\n        if self.test_file:\n            self.test = pl.read_csv(self.test_file)\n            print(\"Training and Testing data loaded successfully.\")\n        else:\n            print(\"Training data loaded successfully.\")\n    \n    def replace_null_values(self):\n        # Replace specific null representations with \"Unknown\"\n        for column in [\"fuel_type\", \"clean_title\", \"accident\", \"transmission\"]:\n            if column in self.train.columns:\n                unique_values = self.train.unique(subset=[column], maintain_order=True)[column]\n                pl_null = unique_values[2] if len(unique_values) > 2 else None  # Adjust index as needed\n                mapping = {'–': pl_null, \"not supported\": pl_null, pl_null: \"Unknown\"}\n                self.train = self.train.with_columns(pl.col(column).replace(mapping))\n                if self.test and column in self.test.columns:\n                    self.test = self.test.with_columns(pl.col(column).replace(mapping))\n                self.train = self.train.with_columns(pl.col(column).replace(pl_null, \"Unknown\"))\n                if self.test and column in self.test.columns:\n                    self.test = self.test.with_columns(pl.col(column).replace(pl_null, \"Unknown\"))\n        print(\"Null values replaced.\")\n\n    def categorize_transmission(self, trans):\n        if trans is None or pd.isnull(trans):\n            return \"Other\"\n        trans = trans.lower()\n        if \"manual\" in trans or \"m/t\" in trans:\n            return \"Manual\"\n        elif \"automatic\" in trans or \"a/t\" in trans or \"cvt\" in trans:\n            return \"Automatic\"\n        else:\n            return \"Other\"\n\n    def extract_speed(self, trans):\n        if trans is None or pd.isnull(trans):\n            return \"Other\"\n        match = re.search(r\"(\\d+)-speed\", trans, re.IGNORECASE)\n        if match:\n            return match.group(1)\n        else:\n            return \"Other\"\n\n    def categorize_transmissions(self):\n        # Apply categorization on training data\n        df_pandas = self.train.to_pandas()\n        df_pandas[\"transmission_category\"] = df_pandas[\"transmission\"].apply(self.categorize_transmission)\n        df_pandas[\"speed_category\"] = df_pandas[\"transmission\"].apply(self.extract_speed)\n        self.train = pl.from_pandas(df_pandas)\n\n        # Apply the same to testing data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            df_test[\"transmission_category\"] = df_test[\"transmission\"].apply(self.categorize_transmission)\n            df_test[\"speed_category\"] = df_test[\"transmission\"].apply(self.extract_speed)\n            self.test = pl.from_pandas(df_test)\n        print(\"Transmissions categorized.\")\n\n    def extract_engine_details(self, engine_str):\n        if pd.isnull(engine_str):\n            return 0.0, 0.0, 0\n        hp_match = re.search(r\"(\\d{2,3}\\.?\\d*)HP\", engine_str)\n        l_match = re.search(r\"(\\d\\.?\\d*)L\", engine_str)\n        cyl_match = re.search(r\"(\\d+) Cylinder\", engine_str)\n        hp = float(hp_match.group(1)) if hp_match else 0.0\n        liters = float(l_match.group(1)) if l_match else 0.0\n        cylinders = int(cyl_match.group(1)) if cyl_match else 0\n        return hp, liters, cylinders\n\n    def apply_engine_extraction(self):\n        # Apply extraction on training data\n        df_pandas = self.train.to_pandas()\n        engine_details = df_pandas['engine'].apply(self.extract_engine_details)\n        df_pandas['HP'], df_pandas['Liters'], df_pandas['Cylinders'] = zip(*engine_details)\n        self.train = pl.from_pandas(df_pandas)\n        self.train = self.train.drop([\"transmission\", \"engine\"])\n        \n        # Apply the same to testing data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            engine_details_test = df_test['engine'].apply(self.extract_engine_details)\n            df_test['HP'], df_test['Liters'], df_test['Cylinders'] = zip(*engine_details_test)\n            self.test = pl.from_pandas(df_test)\n            self.test = self.test.drop([\"transmission\", \"engine\"])\n        print(\"Engine details extracted.\")\n\n    def detect_categorical_columns(self):\n        \"\"\"\n        Automatically detect categorical columns based on data types and additional specifications.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        # Detect categorical columns based on data types\n        categorical_columns = df_pandas.select_dtypes(include=['object', 'category', 'bool', 'string']).columns.tolist()\n        \n        # Include additional categorical columns provided by the user\n        for col in self.additional_categorical:\n            if col in df_pandas.columns and col not in categorical_columns:\n                categorical_columns.append(col)\n        \n        print(f\"Detected categorical columns: {categorical_columns}\")\n        return categorical_columns\n\n    def label_encode_and_handle_rare(self, cat_cols):\n        \"\"\"\n        Label encode categorical columns and handle rare categories.\n        Rare categories are replaced with 0.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        if self.test:\n            df_test = self.test.to_pandas()\n        else:\n            df_test = None\n\n        for c in cat_cols:\n            print(f\"\\nProcessing categorical column: {c}\")\n            # Ensure the column is of a categorical type\n            if df_pandas[c].dtype not in ['object', 'category', 'bool', 'string', 'int64']:\n                print(f\"Skipping column {c} as it is not of a categorical type.\")\n                continue\n            # Factorize to get integer codes\n            df_pandas[c], uniques = pd.factorize(df_pandas[c], sort=True)\n            if df_test is not None:\n                # Apply the same factorization to test data\n                df_test[c] = pd.Categorical(df_test[c], categories=uniques).codes\n                df_test[c] = df_test[c].astype(int)\n            # Shift to ensure minimum label is 0\n            df_pandas[c] -= df_pandas[c].min()\n            if df_test is not None:\n                df_test[c] -= df_pandas[c].min()\n            # Get value counts\n            vc = df_pandas[c].value_counts()\n            # Identify rare categories\n            rare_categories = vc[vc < self.rare_threshold].index.values\n            self.RARE.append(rare_categories)\n            # Number of unique categories excluding rare\n            n_unique = df_pandas[c].nunique()\n            min_val = df_pandas[c].min()\n            max_val = df_pandas[c].max()\n            rare_count = len(rare_categories)\n            print(f'{c}: nunique={n_unique}, min={min_val}, max={max_val}, rare_ct={rare_count}')\n            # Update CAT_SIZE and CAT_EMB\n            # +1 for rare category\n            cat_size = max_val + 2  # Adding one more for rare\n            self.CAT_SIZE.append(cat_size)\n            self.CAT_EMB.append(int(np.ceil(np.sqrt(cat_size))))\n            # Increment labels by 1 to reserve 0 for rare\n            df_pandas[c] += 1\n            if df_test is not None:\n                df_test[c] += 1\n            # Replace rare categories with 0\n            rare_indices = rare_categories + 1  # Since labels have been incremented\n            df_pandas.loc[df_pandas[c].isin(rare_indices), c] = 0\n            if df_test is not None:\n                df_test.loc[df_test[c].isin(rare_indices), c] = 0\n            # Store LabelEncoder (if needed elsewhere)\n            le = LabelEncoder()\n            # Fit on non-rare categories\n            non_rare = df_pandas[df_pandas[c] != 0][c]\n            le.fit(non_rare)\n            self.label_encoders[c] = le\n            print(f\"Label encoding completed for column: {c}\")\n        \n        # Update the training and testing data\n        self.train = pl.from_pandas(df_pandas)\n        if self.test:\n            self.test = pl.from_pandas(df_test)\n        print(\"\\nLabel encoding and rare category handling complete.\")\n\n    def fill_nulls(self):\n        df_pandas = self.train.to_pandas()\n        numerical_cols = ['speed_category', 'HP', 'Liters', 'Cylinders']\n        for col in numerical_cols:\n            if col in df_pandas.columns:\n                df_pandas[col].fillna(0, inplace=True)\n                if self.test and col in self.test.columns:\n                    df_test = self.test.to_pandas()\n                    df_test[col].fillna(0, inplace=True)\n                    self.test = pl.from_pandas(df_test)\n        self.train = pl.from_pandas(df_pandas)\n        print(\"Null values filled.\")\n\n    def drop_columns(self, columns):\n        existing_columns_train = self.train.columns\n        columns_to_drop = [col for col in columns if col in existing_columns_train]\n        self.train = self.train.drop(columns_to_drop)\n        if self.test:\n            existing_columns_test = self.test.columns\n            columns_to_drop_test = [col for col in columns if col in existing_columns_test]\n            self.test = self.test.drop(columns_to_drop_test)\n        print(f\"Dropped columns: {columns_to_drop}\")\n\n    def detect_numerical_columns(self):\n        \"\"\"\n        Detect numerical columns that need to be scaled.\n        \"\"\"\n        df_pandas = self.train.to_pandas()\n        numerical_columns = df_pandas.select_dtypes(include=['float64', 'int64']).columns.tolist()\n        # Exclude categorical columns\n        numerical_columns = [col for col in numerical_columns if col not in self.CATS]\n        # Further exclude target variable if present\n        if 'price' in numerical_columns:\n            numerical_columns.remove('price')\n        self.NUMS = numerical_columns\n        print(f\"Detected numerical columns for scaling: {self.NUMS}\")\n        return self.NUMS\n\n    def scale_features(self):\n        df_pandas = self.train.to_pandas()\n        numerical_columns = self.NUMS\n        print(f\"Scaling numerical columns: {numerical_columns}\")\n        # Fit scaler on training data\n        df_pandas[numerical_columns] = self.scaler.fit_transform(df_pandas[numerical_columns])\n        self.train = pl.from_pandas(df_pandas)\n        \n        # Apply scaler to test data if available\n        if self.test:\n            df_test = self.test.to_pandas()\n            df_test[numerical_columns] = self.scaler.transform(df_test[numerical_columns])\n            self.test = pl.from_pandas(df_test)\n        print(\"Features scaled.\")\n\n    def preprocess(self):\n        self.load_data()\n        self.replace_null_values()\n        self.categorize_transmissions()\n        self.apply_engine_extraction()\n        # Automatically detect categorical columns\n        categorical_columns = self.detect_categorical_columns()\n        self.CATS = categorical_columns\n        self.label_encode_and_handle_rare(cat_cols=categorical_columns)\n        self.fill_nulls()\n        self.drop_columns([\"id\"])  # Ensure 'id' exists or handle if missing\n        self.detect_numerical_columns()\n        self.scale_features()\n        print(\"Preprocessing complete.\")\n\n    def get_embedding_info(self):\n        \"\"\"\n        Returns a dictionary with categorical columns as keys and a tuple of (CAT_SIZE, CAT_EMB)\n        \"\"\"\n        embedding_info = {}\n        for idx, c in enumerate(self.CATS):\n            embedding_info[c] = (self.CAT_SIZE[idx], self.CAT_EMB[idx])\n        return embedding_info\n\n    def print_dataframe(self, dataset='train'):\n        # Display the dataframe\n        if dataset == 'train':\n            return self.train\n        elif dataset == 'test' and self.test is not None:\n            return self.test\n        else:\n            print(f\"No dataset named '{dataset}' found.\")\n            return None\n\n# Example usage:\n# preprocessor = DataPreprocessor(\n#     train_file='train.csv',\n#     test_file='test.csv',\n#     rare_threshold=40,\n#     additional_categorical=['brand', 'model']  # Specify additional categorical columns here\n# )\n# preprocessor.preprocess()\n# processed_train = preprocessor.print_dataframe('train')\n# processed_test = preprocessor.print_dataframe('test')\n# embedding_info = preprocessor.get_embedding_info()\n# print(embedding_info)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile plots.py\n\n\nimport os\nimport re\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport shap\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Plotter:\n    def __init__(self, X, y):\n        \"\"\"\n        Initializes the Plotter with preprocessed and scaled features and target data.\n        \n        Parameters:\n        - X: Feature DataFrame (already scaled)\n        - y: Target Series (already scaled)\n        \"\"\"\n        self.X = X\n        self.y = y\n\n\n    def plot_correlation_matrix(self):\n        \"\"\"\n        Plots the correlation matrix of the combined feature and target data.\n        \"\"\"\n        # Combine X and y into one DataFrame\n        df_combined = pd.concat([self.X, self.y], axis=1)\n\n        # Calculate the correlation matrix\n        corr_matrix = df_combined.corr()\n\n        # Plot the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n        plt.title(\"Correlation Matrix\", fontsize=16)\n        plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels slightly for better readability\n        plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n        plt.tight_layout()\n        plt.show()\n        \n    \n    def plot_feature_importance(self, model):\n        \"\"\"\n        Plots feature importance using the provided trained model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor) with a `feature_importances_` attribute.\n        \"\"\"\n        feature_importances = model.feature_importances_\n        importance_df = pd.DataFrame({\n            'Feature': self.X.columns,\n            'Importance': feature_importances\n        }).sort_values(by='Importance', ascending=False)\n\n        # Plot Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n        plt.title(\"Feature Importance\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_permutation_importance(self, model, random_state=42, n_estimators=100, n_repeats=10):\n        \"\"\"\n        Plots permutation feature importance based on the preprocessed data (already scaled).\n        \n        Parameters:\n        - random_state: Random seed for reproducibility (default 42)\n        - n_estimators: Number of trees in the RandomForestRegressor (default 100)\n        - n_repeats: Number of times to shuffle the data during permutation importance (default 10)\n        \"\"\"\n\n        perm_importance = permutation_importance(model, self.X, self.y, n_repeats=n_repeats, random_state=random_state, scoring='neg_root_mean_squared_error')\n\n        # Step 3: Create a DataFrame to store the results\n        perm_importance_df = pd.DataFrame({\n            'Feature': self.X.columns,  # Use X's original column names\n            'Importance': perm_importance.importances_mean\n        }).sort_values(by='Importance', ascending=False)\n\n        # Step 4: Plot Permutation Feature Importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Feature', data=perm_importance_df, palette='coolwarm')\n        plt.title(\"Permutation Feature Importance (Scaled)\", fontsize=16)\n        plt.tight_layout()\n        plt.show()\n        \n        \n    def plot_shap_summary(self, model):\n        \"\"\"\n        Plots a SHAP summary plot for the provided model.\n        \n        Parameters:\n        - model: A trained model (e.g., RandomForestRegressor)\n        \"\"\"\n        # Create SHAP explainer\n        explainer = shap.TreeExplainer(model)\n\n        # Calculate SHAP values for the feature set\n        shap_values = explainer.shap_values(self.X)\n\n        # Plot SHAP summary plot (global interpretation)\n        shap.summary_plot(shap_values, self.X, feature_names=self.X.columns)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import data_preprocessing\nimport plots\nimport importlib\nimportlib.reload(data_preprocessing)\nimportlib.reload(plots)\nfrom data_preprocessing import DataPreprocessor\nfrom plots import Plotter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATAPATH = \"/kaggle/input/playground-series-s4e9/\"\nTRAIN_SET = DATAPATH + \"train.csv\"\nTEST_SET = DATAPATH + \"test.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = pl.read_csv(TRAIN_SET)\n# y = data[\"price\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage\n# preprocessor = DataPreprocessor(TRAIN_SET)\n# preprocessor.preprocess()\n# X = preprocessor.train.drop(\"price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor.print_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# import tensorflow as tf\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.layers import Dense, Dropout, Input, Embedding\n# from tensorflow.keras.layers import Concatenate, Multiply\n# import tensorflow.keras.backend as K\n\n# print('TF Version',tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EPOCHS = 3\n# LRS = [0.001]*2 + [0.0001]*1\n\n# def lrfn(epoch):\n#     return LRS[epoch]\n\n# rng = [i for i in range(EPOCHS)]\n# lr_y = [lrfn(x) for x in rng]\n# plt.figure(figsize=(10, 4))\n# plt.plot(rng, lr_y, '-o')\n# print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n#         format(lr_y[0], max(lr_y), lr_y[-1]))\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Learning Rate\")\n# plt.title(\"Learning Rate Schedule\")\n# plt.show()\n\n# lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Flatten, Concatenate\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Import the DataPreprocessor class\nfrom data_preprocessing import DataPreprocessor\n\n# Define your version number for OOF predictions\nVER = 1\n\n# Initialize Preprocessor\npreprocessor = DataPreprocessor(\n    train_file=TRAIN_SET,  # Replace with your actual training file path\n#     test_file='test.csv',    # Replace with your actual testing file path\n    rare_threshold=40,\n    additional_categorical=['brand', 'model']  # Specify additional categorical columns here\n)\npreprocessor.preprocess()\n\n# Retrieve processed data\nprocessed_train = preprocessor.print_dataframe('train').to_pandas()\n# processed_test = preprocessor.print_dataframe('test').to_pandas()\nembedding_info = preprocessor.get_embedding_info()\nprint(\"Embedding Info:\", embedding_info)\n\n# Define categorical and numerical columns\nCATS = preprocessor.CATS\nNUMS = preprocessor.NUMS\n\n# Define target\nTARGET = 'price'\n\n# Split the data into training and validation sets (80:20)\nX = processed_train.drop(columns=[TARGET])\ny = processed_train[TARGET].values\n\nX_train_cats, X_valid_cats, X_train_nums, X_valid_nums, y_train, y_valid = train_test_split(\n    X[CATS].values,\n    X[NUMS].values,\n    y,\n    test_size=0.2,\n    random_state=42\n)\n\n# Learning Rate Schedule\nEPOCHS = 15\nLRS = [0.001] * 2 + [0.0001] * 1\n\ndef lrfn(epoch):\n    return LRS[epoch] if epoch < len(LRS) else LRS[-1]\n\n# Plot Learning Rate Schedule\nrng = list(range(EPOCHS))\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(lr_y[0], max(lr_y), lr_y[-1]))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Learning Rate Schedule\")\nplt.show()\n\nlr_callback = LearningRateScheduler(lrfn, verbose=True)\n\n# Build Keras Model\ndef build_model(embedding_info, numerical_input_size):\n    # CATEGORICAL FEATURES\n    x_input_cats = Input(shape=(len(CATS),), name='categorical_input')\n    embs = []\n    for j, cat in enumerate(CATS):\n        num_categories, embed_dim = embedding_info[cat]\n        e = Embedding(input_dim=num_categories, output_dim=embed_dim, name=f'embedding_{cat}')\n        x = e(x_input_cats[:, j])\n        x = Flatten()(x)\n        embs.append(x)\n    \n    # NUMERICAL FEATURES\n    x_input_nums = Input(shape=(numerical_input_size,), name='numerical_input')\n    \n    # COMBINE\n    x = Concatenate(axis=-1)(embs + [x_input_nums]) \n    x = Dense(256, activation='relu')(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(256, activation='relu')(x)\n    output = Dense(1, activation='linear')(x)\n    \n    model = Model(inputs=[x_input_cats, x_input_nums], outputs=output)\n    \n    return model\n\n# Initialize Model\nmodel = build_model(embedding_info, numerical_input_size=len(NUMS))\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lrfn(0)), \n    loss=\"mean_squared_error\", \n    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)\n\n# Display Model Summary\nmodel.summary()\n\n# Train the Model\nhistory = model.fit(\n    [X_train_cats, X_train_nums], y_train, \n    validation_data=([X_valid_cats, X_valid_nums], y_valid),\n    callbacks=[lr_callback],\n    batch_size=64, epochs=EPOCHS, verbose=2\n)\n\n# Plot Training and Validation RMSE\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['root_mean_squared_error'], label='Train RMSE')\nplt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.title('Training and Validation RMSE')\nplt.legend()\nplt.show()\n\n# Predict on Validation Set\noof_preds = model.predict([X_valid_cats, X_valid_nums], batch_size=512).flatten()\n\n# Compute RMSE for Validation Set\nrmse = np.sqrt(np.mean((oof_preds - y_valid) ** 2))\nprint('Validation RMSE =', rmse)\n\n# Compute Overall CV RMSE (Here it's just the validation RMSE since we're not using K-Fold)\nrsme = rmse\nprint(\"Overall CV RMSE =\", rsme)\n\n# Save OOF Predictions\nif 'id' in processed_train.columns:\n    oof_df = processed_train.loc[X_valid_cats[:,0].astype(int), ['id']].copy()  # Adjust indexing as needed\n    oof_df[\"pred\"] = oof_preds\n    oof_df.to_csv(f\"oof_v{VER}.csv\", index=False)\nelse:\n    oof_df = pd.DataFrame({'pred': oof_preds})\n    oof_df.to_csv(f\"oof_v{VER}.csv\", index=False)\n    print(\"Warning: 'id' column not found. OOF predictions saved without 'id'.\")\n\n# Predict on Test Set\nX_test_cats = processed_test[CATS].values\nX_test_nums = processed_test[NUMS].values\ntest_preds = model.predict([X_test_cats, X_test_nums], batch_size=512).flatten()\n\n# Save Test Predictions\nprocessed_test['preds'] = test_preds\nprocessed_test.to_csv('test_with_preds.csv', index=False)\nprint(\"Test predictions saved to 'test_with_preds.csv'\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Dictionary for RandomForestRegressor parameters\n# rf_params = {\n#     'n_estimators': 100,\n#     'criterion': 'squared_error',\n#     'max_depth': None,\n#     'min_samples_split': 2,\n#     'min_samples_leaf': 1,\n#     'min_weight_fraction_leaf': 0.0,\n#     'max_features': 'auto',\n#     'max_leaf_nodes': None,\n#     'bootstrap': True,\n#     'oob_score': False,\n#     'n_jobs': -1,\n#     'random_state': 42,\n#     'verbose': 0,\n#     'warm_start': False,\n#     'ccp_alpha': 0.0,\n#     'max_samples': None\n# }\n\n\n\n# # Dictionary for SVR parameters\n# svr_params = {\n#     'kernel': 'rbf',\n#     'degree': 3,\n#     'gamma': 'scale',\n#     'coef0': 0.0,\n#     'tol': 1e-3,\n#     'C': 1.0,\n#     'epsilon': 0.1,\n#     'shrinking': True,\n#     'cache_size': 200,\n#     'verbose': False,\n#     'max_iter': -1\n# }\n\n\n# xgb_model = xgb.XGBRegressor(\n#     objective='reg:squarederror',\n#     n_estimators=100,\n#     max_depth=6,\n#     learning_rate=0.1,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     random_state=42,\n#     tree_method='gpu_hist'  # Enable GPU\n# )\n\n# lgb_model = lgb.LGBMRegressor(\n#     boosting_type='gbdt',\n#     num_leaves=31,\n#     learning_rate=0.1,\n#     n_estimators=100,\n#     random_state=42,\n#     device='gpu'  # Enable GPU\n# )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_dict = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Instantiate the RandomForestRegressor model using the parameters from the dictionary\n# rf_model = RandomForestRegressor(**rf_params)\n# rf_model.fit(X_train, y_train)\n# rf_predictions = rf_model.predict(X_test)\n# rf_mse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n# models_dict[rf_mse] = rf_model\n# print(f\"RandomForest RMSE: {rf_mse}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Instantiate the SVR model using the parameters from the dictionary\n# svr_model = SVR(**svr_params)\n# svr_model.fit(X_train, y_train)\n# svr_predictions = svr_model.predict(X_test)\n# svr_mse = np.sqrt(mean_squared_error(y_test, svr_predictions))\n# models_dict[svr_mse] = svr_model\n# print(f\"SVR RMSE: {svr_mse}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_model.fit(X_train, y_train)\n# xgb_predictions = xgb_model.predict(X_test)\n# xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n# models_dict[xgb_rmse] = xgb_model  # Store the RMSE and model\n# print(f\"XGBoost RMSE: {xgb_rmse}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_model.fit(X_train, y_train)\n# lgb_predictions = lgb_model.predict(X_test)\n# lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_predictions))\n# models_dict[lgb_rmse] = lgb_model  # Store the RMSE and model\n# print(f\"LightGBM RMSE: {lgb_rmse}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = DataPreprocessor(TEST_SET)\npreprocessor.preprocess()\nprint(\"Test Data Preproccessing complete\")\ntest_data = preprocessor.test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission(test_set, models_dict, TEST_SET = DATAPATH + \"test.csv\", output_file=\"submission.csv\"):\n    \n    \n    best_rmse = min(models_dict.keys())  # Get the smallest RMSE\n    model = models_dict[best_rmse]  # Get the corresponding model\n    print(f\"Best RMSE: {best_rmse}\")\n    # Assuming test_set is a polars DataFrame\n    # If it's not, we can convert it from pandas to polars\n    if isinstance(test_set, pd.DataFrame):\n        test_set = pl.DataFrame(test_set)\n    \n    # Extract the 'id' column\n    ids = pl.read_csv(TEST_SET)[\"id\"]\n\n    # Predict on the entire test set (assuming model.predict() works on a whole dataframe)\n    predictions = model.predict(test_set.to_pandas())  # converting polars DataFrame to pandas for prediction\n\n    # Create a new polars DataFrame with 'id' and 'price' (predictions)\n    submission = pl.DataFrame({\n        'id': ids,\n        'price': predictions\n    })\n\n    # Write to CSV\n    submission.write_csv(output_file)\n    print(f\"Submission file saved as {output_file}\")\n\n\ncreate_submission(test_data, models_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotter = Plotter(X, y)\n# plotter.plot_feature_importance(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### imports","metadata":{}},{"cell_type":"code","source":"# import data_preprocessing\n# import plots\n# import importlib\n# importlib.reload(data_preprocessing)\n# importlib.reload(plots)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile data_preprocessing.py\n\n# import os\n# import re\n# import polars as pl\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n# from sklearn.inspection import permutation_importance\n# import shap\n# import warnings\n\n# # Ignore all warnings\n# warnings.filterwarnings('ignore')\n\n\n# class DataPreprocessor:\n#     def __init__(self, train_file, test_file=None):\n#         self.train_file = train_file\n#         self.test_file = test_file\n#         self.data = None\n#         self.label_encoders = {}\n#         self.scaler = StandardScaler()\n\n#     def load_data(self):\n#         self.data = pl.read_csv(self.train_file)\n#         print(\"Data loaded successfully.\")\n    \n#     def replace_null_values(self):\n#         types = self.data.unique(subset=[\"fuel_type\"], maintain_order=True)[\"fuel_type\"]\n#         pl_null = types[2]\n#         mapping = {'–': pl_null, \"not supported\": pl_null, pl_null: \"Unknown\"}\n#         self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(mapping))\n#         self.data = self.data.with_columns(clean_title=pl.col(\"clean_title\").replace(mapping))\n#         self.data = self.data.with_columns(accident=pl.col(\"accident\").replace(mapping))\n#         self.data = self.data.with_columns(transmission=pl.col(\"transmission\").replace(mapping))\n#         self.data = self.data.with_columns(fuel_type=pl.col(\"fuel_type\").replace(pl_null, \"Unknown\"))\n#         print(\"Null values replaced.\")\n    \n#     def categorize_transmission(self, trans):\n#         if trans is None:\n#             return None\n#         trans = trans.lower()\n#         if \"manual\" in trans or \"m/t\" in trans:\n#             return \"Manual\"\n#         elif \"automatic\" in trans or \"a/t\" in trans or \"cvt\" in trans:\n#             return \"Automatic\"\n#         else:\n#             return \"Other\"\n    \n#     def extract_speed(self, trans):\n#         if trans is None:\n#             return \"Other\"\n#         match = re.search(r\"(\\d+)-speed\", trans, re.IGNORECASE)\n#         if match:\n#             return match.group(1)\n#         else:\n#             return \"Other\"\n    \n#     def categorize_transmissions(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas[\"transmission_category\"] = df_pandas[\"transmission\"].apply(self.categorize_transmission)\n#         df_pandas[\"speed_category\"] = df_pandas[\"transmission\"].apply(self.extract_speed)\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Transmissions categorized.\")\n    \n#     def extract_engine_details(self, engine_str):\n#         if pd.isnull(engine_str):\n#             return None, None, None\n#         hp_match = re.search(r\"(\\d{2,3}\\.?\\d*)HP\", engine_str)\n#         l_match = re.search(r\"(\\d\\.?\\d*)L\", engine_str)\n#         cyl_match = re.search(r\"(\\d+) Cylinder\", engine_str)\n#         hp = hp_match.group(1) if hp_match else None\n#         liters = l_match.group(1) if l_match else None\n#         cylinders = cyl_match.group(1) if cyl_match else None\n#         return hp, liters, cylinders\n\n#     def apply_engine_extraction(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas['HP'], df_pandas['Liters'], df_pandas['Cylinders'] = zip(*df_pandas['engine'].apply(self.extract_engine_details))\n#         self.data = pl.from_pandas(df_pandas)\n#         self.data = self.data.drop(\"transmission\")\n#         self.data = self.data.drop(\"engine\")\n#         print(\"Engine details extracted.\")\n\n#     def label_encode(self):\n#         cat_data = self.data.select([col for col in self.data.columns if not self.data[col].dtype.is_numeric()])\n#         cat_data = cat_data.to_pandas()\n#         for column in cat_data.select_dtypes(include=['object']).columns:\n#             self.label_encoders[column] = LabelEncoder()\n#             cat_data[column] = self.label_encoders[column].fit_transform(cat_data[column])\n#         num_data = self.data.select([col for col in self.data.columns if self.data[col].dtype.is_numeric()])\n#         self.data = pl.from_pandas(pd.concat([num_data.to_pandas(), cat_data], axis=1))\n#         print(\"Categorical columns label encoded.\")\n\n#     def fill_nulls(self):\n#         df_pandas = self.data.to_pandas()\n#         df_pandas['speed_category'].fillna(0, inplace=True)\n#         df_pandas['HP'].fillna(0, inplace=True)\n#         df_pandas['Liters'].fillna(0, inplace=True)\n#         df_pandas['Cylinders'].fillna(0, inplace=True)\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Null values filled.\")\n    \n#     def drop_columns(self, columns):\n#         self.data = self.data.drop(columns)\n#         print(f\"Dropped columns: {columns}\")\n\n#     def scale_features(self):\n#         df_pandas = self.data.to_pandas()\n#         numeric_columns = df_pandas.select_dtypes(include=['float64', 'int64']).columns\n#         df_pandas[numeric_columns] = self.scaler.fit_transform(df_pandas[numeric_columns])\n#         self.data = pl.from_pandas(df_pandas)\n#         print(\"Features scaled.\")\n    \n#     def plot_feature_importance(self, target_column):\n#         X = self.data.drop(target_column).to_pandas()\n#         y = self.data[target_column].to_pandas()\n#         model = RandomForestRegressor(n_estimators=100, random_state=42)\n#         model.fit(X, y)\n#         feature_importances = model.feature_importances_\n#         importance_df = pd.DataFrame({\n#             'Feature': X.columns,\n#             'Importance': feature_importances\n#         }).sort_values(by='Importance', ascending=False)\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n#         plt.title(f\"Feature Importance with respect to '{target_column}'\")\n#         plt.tight_layout()\n#         plt.show()\n\n#     def preprocess(self):\n#         self.load_data()\n#         self.replace_null_values()\n#         self.categorize_transmissions()\n#         self.apply_engine_extraction()\n#         self.label_encode()\n#         self.fill_nulls()\n#         self.drop_columns([\"id\"])\n#         self.scale_features()\n#         print(\"Preprocessing complete.\")\n    \n#     def print_dataframe(self):\n#         # Display the dataframe\n#         return self.data","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile plots.py\n\n\n# import os\n# import re\n# import polars as pl\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n# from sklearn.inspection import permutation_importance\n# import shap\n# import warnings\n\n# # Ignore all warnings\n# warnings.filterwarnings('ignore')\n\n\n# class Plotter:\n#     def __init__(self, X, y):\n#         \"\"\"\n#         Initializes the Plotter with preprocessed and scaled features and target data.\n        \n#         Parameters:\n#         - X: Feature DataFrame (already scaled)\n#         - y: Target Series (already scaled)\n#         \"\"\"\n#         self.X = X\n#         self.y = y\n\n\n#     def plot_correlation_matrix(self):\n#         \"\"\"\n#         Plots the correlation matrix of the combined feature and target data.\n#         \"\"\"\n#         # Combine X and y into one DataFrame\n#         df_combined = pd.concat([self.X, self.y], axis=1)\n\n#         # Calculate the correlation matrix\n#         corr_matrix = df_combined.corr()\n\n#         # Plot the correlation matrix using a heatmap\n#         plt.figure(figsize=(10, 8))\n#         sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns)\n#         plt.title(\"Correlation Matrix\", fontsize=16)\n#         plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels slightly for better readability\n#         plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n#         plt.tight_layout()\n#         plt.show()\n        \n    \n#     def plot_feature_importance(self, model):\n#         \"\"\"\n#         Plots feature importance using the provided trained model.\n        \n#         Parameters:\n#         - model: A trained model (e.g., RandomForestRegressor) with a `feature_importances_` attribute.\n#         \"\"\"\n#         feature_importances = model.feature_importances_\n#         importance_df = pd.DataFrame({\n#             'Feature': self.X.columns,\n#             'Importance': feature_importances\n#         }).sort_values(by='Importance', ascending=False)\n\n#         # Plot Feature Importance\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=importance_df, palette='coolwarm')\n#         plt.title(\"Feature Importance\", fontsize=16)\n#         plt.tight_layout()\n#         plt.show()\n    \n#     def plot_permutation_importance(self, model, random_state=42, n_estimators=100, n_repeats=10):\n#         \"\"\"\n#         Plots permutation feature importance based on the preprocessed data (already scaled).\n        \n#         Parameters:\n#         - random_state: Random seed for reproducibility (default 42)\n#         - n_estimators: Number of trees in the RandomForestRegressor (default 100)\n#         - n_repeats: Number of times to shuffle the data during permutation importance (default 10)\n#         \"\"\"\n\n#         perm_importance = permutation_importance(model, self.X, self.y, n_repeats=n_repeats, random_state=random_state, scoring='neg_root_mean_squared_error')\n\n#         # Step 3: Create a DataFrame to store the results\n#         perm_importance_df = pd.DataFrame({\n#             'Feature': self.X.columns,  # Use X's original column names\n#             'Importance': perm_importance.importances_mean\n#         }).sort_values(by='Importance', ascending=False)\n\n#         # Step 4: Plot Permutation Feature Importance\n#         plt.figure(figsize=(10, 6))\n#         sns.barplot(x='Importance', y='Feature', data=perm_importance_df, palette='coolwarm')\n#         plt.title(\"Permutation Feature Importance (Scaled)\", fontsize=16)\n#         plt.tight_layout()\n#         plt.show()\n        \n        \n#     def plot_shap_summary(self, model):\n#         \"\"\"\n#         Plots a SHAP summary plot for the provided model.\n        \n#         Parameters:\n#         - model: A trained model (e.g., RandomForestRegressor)\n#         \"\"\"\n#         # Create SHAP explainer\n#         explainer = shap.TreeExplainer(model)\n\n#         # Calculate SHAP values for the feature set\n#         shap_values = explainer.shap_values(self.X)\n\n#         # Plot SHAP summary plot (global interpretation)\n#         shap.summary_plot(shap_values, self.X, feature_names=self.X.columns)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}